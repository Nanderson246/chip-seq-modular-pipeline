[
  {
    "objectID": "content/troubleshooting.html",
    "href": "content/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "â— Troubleshooting / FAQâ€\n\nğŸ” Metadata Validation Fails\nğŸ›‘ IDR Errors: NumPy or Missing Peaks\nâš ï¸ Docker Mount Issues Troubleshooting & FAQ **\nğŸ” Metadata Validation Fails\nğŸ›‘ IDR Errors: NumPy or Missing Peaks 4.3 âš ï¸ Docker Mount Issues",
    "crumbs": [
      "Troubleshooting"
    ]
  },
  {
    "objectID": "content/software.html",
    "href": "content/software.html",
    "title": "ğŸ§° Software Requirements (Non-Docker Setup)",
    "section": "",
    "text": "If youâ€™re running the pipeline without Docker, your system must have the following tools installed and available in your PATH â€” except for picard.jar and qualimap, which are already bundled in the pipeline under tools/ and should remain there.\n\n\nInstall the following tools via your package manager (apt, brew, conda, etc.) or from source as needed:\n\n\n\n\n\n\n\n\nTool\nMinimum Version\nPurpose\n\n\n\n\nbash\n4.0+\nShell scripting\n\n\ncutadapt\n4.0+\nAdapter trimming\n\n\nbwa\n0.7.17+\nRead alignment\n\n\nsamtools\n1.14+\nBAM/SAM manipulation\n\n\npicard\n2.26.10+\nBAM post-processing\n\n\nmacs3\n3.0.0a6+\nPeak calling\n\n\nhomer\nv4.11+\nAlternative peak calling\n\n\npython3\n3.9+\nUsed for helper scripts\n\n\npip\nLatest\nPython package management\n\n\nR\n4.2+\nStatistical computing environment\n\n\nRscript\n4.2+\nScript execution for R modules\n\n\nbedtools\n2.30.0+\nGenomic interval operations\n\n\ndeepTools\n3.5.1+\nQC and coverage tools\n\n\nidr\n2.0.4.2 (patched)\nPeak reproducibility scoring\n\n\njq\n1.6+\nJSON parsing (metadata scripts)\n\n\nwget/curl\nany\nData downloading\n\n\nyq\n4.0+\nYAML parsing and editing (Go-based)\n\n\n\n\n\n\nInstall with:\npip install numpy pandas pyyaml\n\n\n\nYour pipeline includes scripts that require the following CRAN and Bioconductor packages:\n\n\n\ndplyr\nggplot2\ntidyr\ndata.table\nreadr\nstringr\ntools\njsonlite\nhttr\n\n\n\n\n\nclusterProfiler\nenrichplot\nReactomePA\nreactome.db\nbiomaRt\nAnnotationDbi\norg.Hs.eg.db\norg.Mm.eg.db\n\n\nâœ… The R scripts automatically attempt to install missing packages if your internet connection is available.\n\n\n\n\n\n\nFastQC\nMultiQC\nIGV (for manual visualization)\ndocker (if youâ€™d like to use pre-built containers)",
    "crumbs": [
      "ğŸ§° Software Requirements (Non-Docker Setup)"
    ]
  },
  {
    "objectID": "content/software.html#software-requirements-non-docker-setup",
    "href": "content/software.html#software-requirements-non-docker-setup",
    "title": "ğŸ§° Software Requirements (Non-Docker Setup)",
    "section": "",
    "text": "If youâ€™re running the pipeline without Docker, your system must have the following tools installed and available in your PATH â€” except for picard.jar and qualimap, which are already bundled in the pipeline under tools/ and should remain there.\n\n\nInstall the following tools via your package manager (apt, brew, conda, etc.) or from source as needed:\n\n\n\n\n\n\n\n\nTool\nMinimum Version\nPurpose\n\n\n\n\nbash\n4.0+\nShell scripting\n\n\ncutadapt\n4.0+\nAdapter trimming\n\n\nbwa\n0.7.17+\nRead alignment\n\n\nsamtools\n1.14+\nBAM/SAM manipulation\n\n\npicard\n2.26.10+\nBAM post-processing\n\n\nmacs3\n3.0.0a6+\nPeak calling\n\n\nhomer\nv4.11+\nAlternative peak calling\n\n\npython3\n3.9+\nUsed for helper scripts\n\n\npip\nLatest\nPython package management\n\n\nR\n4.2+\nStatistical computing environment\n\n\nRscript\n4.2+\nScript execution for R modules\n\n\nbedtools\n2.30.0+\nGenomic interval operations\n\n\ndeepTools\n3.5.1+\nQC and coverage tools\n\n\nidr\n2.0.4.2 (patched)\nPeak reproducibility scoring\n\n\njq\n1.6+\nJSON parsing (metadata scripts)\n\n\nwget/curl\nany\nData downloading\n\n\nyq\n4.0+\nYAML parsing and editing (Go-based)\n\n\n\n\n\n\nInstall with:\npip install numpy pandas pyyaml\n\n\n\nYour pipeline includes scripts that require the following CRAN and Bioconductor packages:\n\n\n\ndplyr\nggplot2\ntidyr\ndata.table\nreadr\nstringr\ntools\njsonlite\nhttr\n\n\n\n\n\nclusterProfiler\nenrichplot\nReactomePA\nreactome.db\nbiomaRt\nAnnotationDbi\norg.Hs.eg.db\norg.Mm.eg.db\n\n\nâœ… The R scripts automatically attempt to install missing packages if your internet connection is available.\n\n\n\n\n\n\nFastQC\nMultiQC\nIGV (for manual visualization)\ndocker (if youâ€™d like to use pre-built containers)",
    "crumbs": [
      "ğŸ§° Software Requirements (Non-Docker Setup)"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html",
    "href": "content/docker_pipeline2.html",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "",
    "text": "0.1 ğŸ‘·â€ï¸â›ï¸ 1. Build the Docker Image\nFrom the folder containing Dockerfile.pipeline2, run:\ndocker build -f Dockerfile.pipeline2 -t pipeline2-image .\n\n-f: specifies the Dockerfile to use.\n-t: tags the image name as pipeline2-image.\n.: tells Docker to build using the current directory context.\n\n\n\n\n0.2 ğŸ“ 2. Project Structure\nA typical folder layout:\nyour_project/\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/\nâ”‚   â””â”€â”€ pipeline2/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ templates/\nâ”œâ”€â”€ logs/\nâ”œâ”€â”€ analysis/           # Will be populated by the pipeline\nEnsure all these folders exist, especially Reference/, metadata/, and modules/.\n\n\n\n0.3 ğŸ—ƒï¸ 3. Mounting Folders\nTo give Docker access to your files, bind-mount these folders:\n\n\n\n\n\n\n\n\nHost Path\nDocker Path\nReason\n\n\n\n\n$PWD\n/pipeline2\nRoot of your pipeline package\n\n\n$PWD/Reference\n/pipeline2/Reference\nGenome files (FASTA, GTF, BED)\n\n\n$PWD/metadata\n/pipeline2/metadata\nSample metadata for input\n\n\n$PWD/templates\n/pipeline2/templates\nMapping schema, rules\n\n\n$PWD/logs\n/pipeline2/logs\nStore logs\n\n\n$PWD/analysis\n/pipeline2/analysis\nAll analysis results are saved here\n\n\n$PWD/assets\n/pipeline2/assets\nContains images icon for the IDR\n\n\n\n\nreport. Can be changed by user\n\n\n\n\n\n\n\n\n\n\n0.4 ğŸƒ 4. Run the Pipeline\nExample command:\ndocker run --rm \\\n  -v \"$PWD:/pipeline2\" \\\n  -v \"$PWD/Reference:/pipeline2/Reference\" \\\n  -v \"$PWD/metadata:/pipeline2/metadata\" \\\n  -v \"$PWD/templates:/pipeline2/templates\" \\\n  -v \"$PWD/logs:/pipeline2/logs\" \\\n  -v \"$PWD/analysis:/pipeline2/analysis\" \\\n  -w /pipeline2 \\\n  pipeline2-image \\\n  bash run_pipeline2.sh --caller macs3 -genome mm10\n\n\n\n0.5 ğŸ“¦ 5. Output Files\nAfter running, the results will be in:\n\nanalysis/ â€” contains pipeline outputs and QC summaries\nlogs/ â€” contains module-specific logs\nmetadata/mapping_filtered_IDR.tsv â€” filtered metadata after validation\n\n\n\n\n0.6 ğŸ› ï¸ï¸ï¸ 6. Optional: Helper Shell Script\nCreate a script like run.sh:\n#!/bin/bash\ndocker run --rm \\\n  -v \"$PWD:/pipeline2\" \\\n  -v \"$PWD/Reference:/pipeline2/Reference\" \\\n  -v \"$PWD/metadata:/pipeline2/metadata\" \\\n  -v \"$PWD/templates:/pipeline2/templates\" \\\n  -v \"$PWD/logs:/pipeline2/logs\" \\\n  -v \"$PWD/analysis:/pipeline2/analysis\" \\\n  -w /pipeline2 \\\n  pipeline2-image \\\n  bash run_pipeline2.sh \"$@\"\nThen run it like this:\nchmod +x run.sh\n./run.sh --caller homer -genome mm10",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/citations.html",
    "href": "content/citations.html",
    "title": "ğŸ“š Tool Citations",
    "section": "",
    "text": "We respectfully acknowledge the following software used in this pipeline:\n\nCutadapt â€” Martin (2011) DOI: 10.14806/ej.17.1.200\n\nBWA â€” Li and Durbin (2009) PMID: 19451168\n\nSAMtools â€” Li et al.Â (2009) PMID: 19505943\n\nMACS3 â€” Zhang et al.Â (2008) PMID: 18798982\n\nHOMER â€” Heinz et al.Â (2010) PMID: 20513432\n\nPicard â€” Broad Institute https://broadinstitute.github.io/picard/\n\ndeepTools â€” RamÃ­rez et al.Â (2016) PMID: 27079975\n\nQualimap â€” Okonechnikov et al.Â (2016) PMID: 27040156\n\nIDR â€” Li et al.Â (2011) PMID: 21300883\n\nclusterProfiler â€” Yu et al.Â (2012) PMID: 22455463\n\nReactomePA â€” Yu and He (2016) PMID: 27480138\n\nyq â€” Mike Farah (v4+), GitHub: https://github.com/mikefarah/yq\n\nAnd others listed in the software requirements section above.\n\nâš ï¸ For academic use, please cite the original tools if you rely on results from this pipeline.",
    "crumbs": [
      "ğŸ“š Tool Citations"
    ]
  },
  {
    "objectID": "content/mapping.html",
    "href": "content/mapping.html",
    "title": "Preprocessing Modules",
    "section": "",
    "text": "The metadata file is crucial for automating sample grouping, identifying replicates, and selecting the appropriate peak caller.\n\nğŸ§  Filename required: metadata/mapping.tsv\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nSample_ID\nUnique ID matching FASTQ or BAM files\nSRR123456\n\n\nInstrument\nSequencing instrument or platform\nIllumina\n\n\nSample_Type\nChIP, Input, IgG, Mock, etc.\nChIP\n\n\nCondition\nExperimental condition or group\nTreated, WT\n\n\nReplicate\nReplicate number (1, 2, 3, â€¦)\n1\n\n\nTarget\nTranscription factor or histone mark\nH3K27ac, CTCF\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nCell_line\nCell line used\nHEK293\n\n\nSpike_Type\nIf spike-in is used (e.g., dm6, ERCC)\ndm6\n\n\n\n\n\n\nSample_ID   Instrument  Sample_Type Condition   Replicate   Target  Cell_line   Spike_Type\nSRR001      Illumina    ChIP        WT          1         CTCF       HEK293     dm6\nSRR002      Illumina    ChIP        WT          2         CTCF       HEK293     dm6\nSRR003      Illumina    Input       WT          1         None       HEK293     dm6\nSRR004      Illumina    ChIP        KO          1         H3K27me3   HEK293     dm6\nSRR005      Illumina    ChIP        KO          2         H3K27me3   HEK293     dm6\nSRR006      Illumina    Input       KO          1         None       HEK293     dm6\n\n\n\n\nMake sure your file is valid before starting the pipeline:\n\nValidate schema:\n\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\nValidate metadata:\n\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\nâœ”ï¸ You should see â€œValidation Passedâ€ if everything is correct.",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/mapping.html#creating-the-mapping.tsv-file-metadata-table",
    "href": "content/mapping.html#creating-the-mapping.tsv-file-metadata-table",
    "title": "Preprocessing Modules",
    "section": "",
    "text": "The metadata file is crucial for automating sample grouping, identifying replicates, and selecting the appropriate peak caller.\n\nğŸ§  Filename required: metadata/mapping.tsv\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nSample_ID\nUnique ID matching FASTQ or BAM files\nSRR123456\n\n\nInstrument\nSequencing instrument or platform\nIllumina\n\n\nSample_Type\nChIP, Input, IgG, Mock, etc.\nChIP\n\n\nCondition\nExperimental condition or group\nTreated, WT\n\n\nReplicate\nReplicate number (1, 2, 3, â€¦)\n1\n\n\nTarget\nTranscription factor or histone mark\nH3K27ac, CTCF\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nCell_line\nCell line used\nHEK293\n\n\nSpike_Type\nIf spike-in is used (e.g., dm6, ERCC)\ndm6\n\n\n\n\n\n\nSample_ID   Instrument  Sample_Type Condition   Replicate   Target  Cell_line   Spike_Type\nSRR001      Illumina    ChIP        WT          1         CTCF       HEK293     dm6\nSRR002      Illumina    ChIP        WT          2         CTCF       HEK293     dm6\nSRR003      Illumina    Input       WT          1         None       HEK293     dm6\nSRR004      Illumina    ChIP        KO          1         H3K27me3   HEK293     dm6\nSRR005      Illumina    ChIP        KO          2         H3K27me3   HEK293     dm6\nSRR006      Illumina    Input       KO          1         None       HEK293     dm6\n\n\n\n\nMake sure your file is valid before starting the pipeline:\n\nValidate schema:\n\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\nValidate metadata:\n\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\nâœ”ï¸ You should see â€œValidation Passedâ€ if everything is correct.",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/mapping.html#what-is-mapping.tsv-and-why-is-it-important",
    "href": "content/mapping.html#what-is-mapping.tsv-and-why-is-it-important",
    "title": "Preprocessing Modules",
    "section": "2 ğŸ“‹ What is mapping.tsv and Why Is It Important?",
    "text": "2 ğŸ“‹ What is mapping.tsv and Why Is It Important?\n\n2.1 ğŸ” What is it?\nThe mapping.tsv is a metadata tableâ€”a plain text file in tab-separated formatâ€”that contains structured information about each sample in your ChIP-seq experiment. Each row represents a sequencing sample, and each column provides key attributes (e.g., sample ID, condition, replicate number, target protein, etc.).\nThis file needs to be stored at:\nmetadata/mapping.tsv\nThis mapping table will be transformed during the pipeline process: at then you will have a backup (mapping.tsv.bak), mapping_filtered.tsv (produce after replicates QC), and mapping_scaled.tsv (produced after bam cleaning, it contains the ratio of exogenous spike , if spike was detected, and the spike genome type).\n\n\n2.2 ğŸ§  Why is it needed?\nThis file is critical for enabling your pipeline to:\nâœ… Recognize and organize samples automatically âœ… Group replicates (for IDR and reproducibility checks) âœ… Assign controls (Input, IgG, Mock) correctly for peak calling âœ… Select peak caller styles (e.g., narrowPeak for TFs, broadPeak for histone marks) âœ… Validate input consistency using the YAML schema\nWithout this file, the pipeline wouldnâ€™t know how your samples relate to each otherâ€”or how to process them correctly.\n\n\n2.3 ğŸ”— How does the pipeline use it?\n\nPipeline 1: The script 11_Renaming_bam.sh uses the metadata to automatically rename BAM files and organize them into logical groups.\nPipeline 2: Several scripts (e.g., 01_replicate_qc.sh, 03_1_MACS3_peak_calling.sh) read mapping.tsv to apply the right rules for:\n\nSample pairing (ChIP vs.Â Input)\nReplicate merging\nPeak calling type\nReproducibility evaluation (IDR)\n\nValidation scripts (validate_mapping.py, validate_mapping_yaml.sh) ensure the file is complete and follows strict format rules before any analysis starts.\n\n\n\n2.4 ğŸ§¬ Summary\n\n\n\n\n\n\n\nFeature\nWhy it matters\n\n\n\n\nAutomation\nRemoves manual handling of sample groups\n\n\nReproducibility\nEnsures the same logic applies every time\n\n\nCompatibility\nLets the pipeline work across diverse experimental designs\n\n\nQuality control\nPrevents broken analyses due to misannotated samples",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/overview.html",
    "href": "content/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This pipeline was developed and tested on Linux with POSIX-compliant behavior, macOS compatibility is not supported, If adjustments are needed for your system, you are free to modify the setup accordingly. This is the first version and still in beta test. However you can download and tested in your system as full package or use every modular script. The goal was to create a first Github project to establish Bioinformatic competence by the author.\nThis project implements a modular and reproducible ChIP-seq analysis pipeline designed for both human and mouse samples. The pipeline is organized into two main parts:\n\nPreprocessing and BAM Cleaning â€“ Modular pipeline for data preparation.\nPeak Calling and Reproducibility Analysis â€“ Peak detection, quality control, and IDR-based reproducibility assessment.\n\nThe pipeline supports both HOMER and MACS3 workflows, and uses metadata-based automation. Its structure mirrors the ENCODE ChIP-seq processing pipeline, providing a modular, standardized workflow to produce reproducible peak calls, generate signal tracks, and perform quality assessments such as IDR (Irreproducible Discovery Rate), while automating sample naming and grouping.\nThis package is intended to be flexible and modular. Each module can be customized by the user, or the pipeline can be run as-is. It is not a beginner-friendly pipeline, nor was it designed to be. The pipeline runs directly in Bash on a Linux system. Docker files are included to allow execution within containers, and a brief guide is provided at the end of this text, explaining how to build the container and mount the required folders properly.\nTo run this pipeline you will need to have your paired fastq.gz samples in the samples folder, a mapping.tsv data, and to set the reference files in the Reference folder and the spike reference genomes in the SpikeinReference folder.\nThe package provide two dataset test from GEO dataset to test Hg38 and mm10 samples.\nInformation about these two GEO datasets is provided in the README.md file inside the examples/ folder. The pipeline1 is adapted to automatically download an SRR_Acc_List.txt if the list in the metadata folder. We also provide the mapping.tsv as example",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#overview",
    "href": "content/overview.html#overview",
    "title": "Overview",
    "section": "",
    "text": "This pipeline was developed and tested on Linux with POSIX-compliant behavior, macOS compatibility is not supported, If adjustments are needed for your system, you are free to modify the setup accordingly. This is the first version and still in beta test. However you can download and tested in your system as full package or use every modular script. The goal was to create a first Github project to establish Bioinformatic competence by the author.\nThis project implements a modular and reproducible ChIP-seq analysis pipeline designed for both human and mouse samples. The pipeline is organized into two main parts:\n\nPreprocessing and BAM Cleaning â€“ Modular pipeline for data preparation.\nPeak Calling and Reproducibility Analysis â€“ Peak detection, quality control, and IDR-based reproducibility assessment.\n\nThe pipeline supports both HOMER and MACS3 workflows, and uses metadata-based automation. Its structure mirrors the ENCODE ChIP-seq processing pipeline, providing a modular, standardized workflow to produce reproducible peak calls, generate signal tracks, and perform quality assessments such as IDR (Irreproducible Discovery Rate), while automating sample naming and grouping.\nThis package is intended to be flexible and modular. Each module can be customized by the user, or the pipeline can be run as-is. It is not a beginner-friendly pipeline, nor was it designed to be. The pipeline runs directly in Bash on a Linux system. Docker files are included to allow execution within containers, and a brief guide is provided at the end of this text, explaining how to build the container and mount the required folders properly.\nTo run this pipeline you will need to have your paired fastq.gz samples in the samples folder, a mapping.tsv data, and to set the reference files in the Reference folder and the spike reference genomes in the SpikeinReference folder.\nThe package provide two dataset test from GEO dataset to test Hg38 and mm10 samples.\nInformation about these two GEO datasets is provided in the README.md file inside the examples/ folder. The pipeline1 is adapted to automatically download an SRR_Acc_List.txt if the list in the metadata folder. We also provide the mapping.tsv as example",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#pipeline-components",
    "href": "content/overview.html#pipeline-components",
    "title": "Overview",
    "section": "2 ğŸ“š Pipeline Components",
    "text": "2 ğŸ“š Pipeline Components\n\n2.1 ğŸ”§ 1. Read Preprocessing and Alignment\nInput: FASTQ files (reads)\nTools Used:\n\ncutadapt: Adapter trimming\nBWA: Alignment to reference genome\nSamtools, Picard: Sorting, deduplication, indexing, BAM cleanup\n\nOutputs:\n\nUnfiltered alignments\nFiltered, deduplicated BAM files\nRemoval of excluded/blacklisted regions (e.g., ENCODE blacklist)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#replicate-quality-control-with-deeptools",
    "href": "content/overview.html#replicate-quality-control-with-deeptools",
    "title": "Overview",
    "section": "3 ğŸ”¬ Replicate Quality Control with deepTools",
    "text": "3 ğŸ”¬ Replicate Quality Control with deepTools\nThis pipeline uses deepTools (e.g., plotFingerprint, multiBamSummary) to evaluate replicate quality.\n\n3.1 âœ… Advantages of deepTools:\n\nğŸ§ª Modern and actively maintained â€“ compatible with recent Python environments\nğŸ“Š Fingerprint and cross-correlation plots â€“ assess ChIP enrichment and background\nâš™ï¸ Format flexibility â€“ supports BAM and BigWig\nğŸ³ Container-friendly â€“ easy to integrate in Docker-based workflows\nğŸ“ Multi-sample analysis â€“ supports multiBamSummary, plotCorrelation\n\nUsers can optionally replace this with Relative Strand Correlation (RSC) via the ChIPQC R package by editing the relevant module â€” showcasing the pipelineâ€™s modularity.\n\n\n\n3.2 â“ Why not use phantompeakqualtools?\nAlthough once widely used to assess replicate quality, phantompeakqualtools is now:\n\nğŸ›‘ Outdated and no longer actively maintained\nğŸª Depends on legacy Perl environments and outdated tools like CClang v5\nğŸ§± Hard to containerize or integrate into modern pipelines\n\nTherefore, it has been replaced by more maintainable alternatives in this workflow.\n\n\n\n3.3 ğŸ“ˆ 2. Signal Generation\nTools:\n\nMACS3: Generates signal tracks (bedGraph or bigWig)\nBEDTools: For further manipulation\n\nOutputs:\n\nSignal p-value tracks\nFold change over control tracks\n\n\n\n\n3.4 ğŸ“ 3. Peak Calling\nPerformed on:\n\nIndividual replicates\nPooled replicates\nPseudoreplicates\n\nTools:\n\nMACS3 or HOMER\n\nOutputs:\n\nReplicated peaks\nPooled peaks\nPseudoreplicated peaks\n\n\n\n\n3.5 ğŸ“Š 4. IDR Analysis\nTool: idr â€” used to assess peak reproducibility\nThis pipeline uses a patched IDR version compatible with the latest numpy, avoiding the need to downgrade.\n\n\n\n3.6 ğŸ›  Patch + Install IDR (NumPy Compatibility)\nbash\n    wget https://github.com/kundajelab/idr/archive/refs/tags/2.0.4.2.tar.gz && \\\n    tar -xvf 2.0.4.2.tar.gz && \\\n    cd idr-2.0.4.2/idr && \\\n    sed -i 's/numpy.int/int/g' idr.py && \\\n    cd .. && \\\n    pip install . --break-system-packages && \\\n    rm -rf /opt/idr-2.0.4.2 /opt/2.0.4.2.tar.gz\nOutputs:\n\nIDR-ranked peaks\nIDR-thresholded peaks\nConservative IDR peaks\n\n\n\n\n3.7 ğŸ” 5. Replicate/Partition Concordance\nAssesses consistency of peaks across replicates and pseudoreplicates.\n\n\n\n3.8 ğŸ§¾ 6. Format Conversion and Final Outputs\nCovers conversion to final formats for visualization, downstream analysis, or submission.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/installation.html",
    "href": "content/installation.html",
    "title": "âš™ï¸ Installation Guide",
    "section": "",
    "text": "This guide explains how to set up the ChIP-seq Modular Analysis Pipeline using:\n\nâœ… Git clone (recommended)\nğŸ“ .tar.gz or .zip archive download\nğŸ³ Docker (see Docker Guide)\nğŸ§ª Conda environments (see Conda Setup)",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#installation-overview",
    "href": "content/installation.html#installation-overview",
    "title": "âš™ï¸ Installation Guide",
    "section": "",
    "text": "This guide explains how to set up the ChIP-seq Modular Analysis Pipeline using:\n\nâœ… Git clone (recommended)\nğŸ“ .tar.gz or .zip archive download\nğŸ³ Docker (see Docker Guide)\nğŸ§ª Conda environments (see Conda Setup)",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#option-1-clone-from-github",
    "href": "content/installation.html#option-1-clone-from-github",
    "title": "âš™ï¸ Installation Guide",
    "section": "2 ğŸ“‚ Option 1: Clone from GitHub",
    "text": "2 ğŸ“‚ Option 1: Clone from GitHub\nThe most robust and update-safe method:\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\n\n2.1 âœ… Make Scripts Executable (already github executable commit, but in case)\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#option-2-download-archive",
    "href": "content/installation.html#option-2-download-archive",
    "title": "âš™ï¸ Installation Guide",
    "section": "3 ğŸ“¦ Option 2: Download Archive",
    "text": "3 ğŸ“¦ Option 2: Download Archive\n\n3.1 ğŸ”¸ If you downloaded the .tar.gz version:\ntar -xzvf your-pipeline.tar.gz\ncd your-pipeline\nScripts remain executable â€” no chmod needed.\n\n\n3.2 âš ï¸ If you downloaded the .zip version:\nunzip your-pipeline.zip\ncd your-pipeline\nThen run:\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#test-the-installation",
    "href": "content/installation.html#test-the-installation",
    "title": "âš™ï¸ Installation Guide",
    "section": "4 âœ… Test the Installation",
    "text": "4 âœ… Test the Installation\nMake sure the scripts work:\n./run_pipeline1.sh --help\n./run_pipeline2.sh --help",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#expected-file-structure",
    "href": "content/installation.html#expected-file-structure",
    "title": "âš™ï¸ Installation Guide",
    "section": "5 ğŸ“ Expected File Structure",
    "text": "5 ğŸ“ Expected File Structure\nyour-repo/\nâ”œâ”€â”€ run_pipeline1.sh\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/\nâ”‚   â””â”€â”€ pipeline1/\nâ”‚   â””â”€â”€ pipeline2/\nâ”‚   â””â”€â”€ utils/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ analysis/\nâ””â”€â”€ ...",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#related-setup-pages",
    "href": "content/installation.html#related-setup-pages",
    "title": "âš™ï¸ Installation Guide",
    "section": "6 ğŸ“š Related Setup Pages",
    "text": "6 ğŸ“š Related Setup Pages\n\nğŸ“¦ Reference Setup\nğŸ§¬ Metadata & Mapping Format\nğŸ”§ Software Requirements\nğŸ³ Docker Setup\nğŸ§ª Conda Environment",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#notes",
    "href": "content/installation.html#notes",
    "title": "âš™ï¸ Installation Guide",
    "section": "7 ğŸ“ Notes",
    "text": "7 ğŸ“ Notes\nIf you encounter permission errors or missing dependencies, refer to Troubleshooting.\n\n---\n\nLet me know if you want to include a section for installing a local Conda environment (`environment.yml`) or a helper `install.sh`.",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html",
    "href": "content/docker_pipeline1.html",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "",
    "text": "ğŸ³ Docker Usage Guide\nThis guide explains how to build and run the ChIP-seq processing pipeline using Docker. It is designed for users at all levels, including beginners.\nWhile the original pipeline was created to work directly in Host-based installation, the pipeline can be used with docker containers and mounting the required folder needed for the package This pipeline is fully containerized and can be run with a single Docker command. Below is the recommended usage to preserve outputs, access your own data, and avoid re-downloading references.\n\n\n\n\nDocker installed and working (docker --version)\nYour working directory should contain:\n\nReference/ â€” genome files (e.g.Â hg38, mm10, etc.)\nmetadata/ â€” sample sheet (SRR_Acc_List.txt, etc.)\nsamples/ â€” (optional) pre-existing FASTQs\nresults/ â€” (created if missing) final output goes here\nlogs/ â€” (created if missing) all logs go here\n\n\n\n\n\n\ndocker run --rm \\\n  -v \"$PWD/Reference\":/pipeline/Reference \\\n  -v \"$PWD/SpikeinReference\":/pipeline/SpikeinReference \\\n  -v \"$PWD/metadata\":/pipeline/metadata \\\n  -v \"$PWD/samples\":/pipeline/samples \\\n  -v \"$PWD/results\":/pipeline/results \\\n  -v \"$PWD/logs\":/pipeline/logs \\\n  -v \"$PWD/logs\":/pipeline/assets \\\n  pipeline1:latest -t 4 -r mm10 -a tn5_truseq\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--rm\nRemoves container after run\n\n\n-v &lt;host&gt;:&lt;container&gt;\nMounts folders into container\n\n\n-t 4\nNumber of threads\n\n\n-r mm10\nReference genome (hg38, mm10, etc.)\n\n\n-a tn5_truseq\nAdapter type (should match whatâ€™s in your pipeline)\n\n\npipeline1:latest\nDocker image name\n\n\n\n\n\n\n\nAll outputs go into your local results/ and logs/ folders â€” no data is lost after the container exits.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#running-pipeline1-with-docker",
    "href": "content/docker_pipeline1.html#running-pipeline1-with-docker",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "",
    "text": "ğŸ³ Docker Usage Guide\nThis guide explains how to build and run the ChIP-seq processing pipeline using Docker. It is designed for users at all levels, including beginners.\nWhile the original pipeline was created to work directly in Host-based installation, the pipeline can be used with docker containers and mounting the required folder needed for the package This pipeline is fully containerized and can be run with a single Docker command. Below is the recommended usage to preserve outputs, access your own data, and avoid re-downloading references.\n\n\n\n\nDocker installed and working (docker --version)\nYour working directory should contain:\n\nReference/ â€” genome files (e.g.Â hg38, mm10, etc.)\nmetadata/ â€” sample sheet (SRR_Acc_List.txt, etc.)\nsamples/ â€” (optional) pre-existing FASTQs\nresults/ â€” (created if missing) final output goes here\nlogs/ â€” (created if missing) all logs go here\n\n\n\n\n\n\ndocker run --rm \\\n  -v \"$PWD/Reference\":/pipeline/Reference \\\n  -v \"$PWD/SpikeinReference\":/pipeline/SpikeinReference \\\n  -v \"$PWD/metadata\":/pipeline/metadata \\\n  -v \"$PWD/samples\":/pipeline/samples \\\n  -v \"$PWD/results\":/pipeline/results \\\n  -v \"$PWD/logs\":/pipeline/logs \\\n  -v \"$PWD/logs\":/pipeline/assets \\\n  pipeline1:latest -t 4 -r mm10 -a tn5_truseq\n\n\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--rm\nRemoves container after run\n\n\n-v &lt;host&gt;:&lt;container&gt;\nMounts folders into container\n\n\n-t 4\nNumber of threads\n\n\n-r mm10\nReference genome (hg38, mm10, etc.)\n\n\n-a tn5_truseq\nAdapter type (should match whatâ€™s in your pipeline)\n\n\npipeline1:latest\nDocker image name\n\n\n\n\n\n\n\nAll outputs go into your local results/ and logs/ folders â€” no data is lost after the container exits.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "Welcome to the ChIP-seq Modular Analysis Pipeline documentation!\nThis site provides a detailed manual for running and understanding the modular ChIP-seq pipeline developed by Nancy Anderson. It includes setup, usage, modules, metadata structure, Docker/Conda environments, and more.\n\n\n\n\nOverview\nPipeline 1: QC & Preprocessing\nPipeline 2: Peak Calling & Analysis\nMetadata Guide\nDocker Usage - Pipeline 1\nDocker Usage - Pipeline 2\nUnified Docker Guide\nConda Environment Setup\nSoftware Requirements\nTroubleshooting\nCitations\nLicense\n\n\n\n\n\nIf youâ€™re eager to run the pipeline quickly, jump to:\nğŸ‘‰ Quick Start Guide\nOr explore the Docker options for portable setup:\nğŸ‘‰ Docker Setup",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/index.html#sections",
    "href": "content/index.html#sections",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "Overview\nPipeline 1: QC & Preprocessing\nPipeline 2: Peak Calling & Analysis\nMetadata Guide\nDocker Usage - Pipeline 1\nDocker Usage - Pipeline 2\nUnified Docker Guide\nConda Environment Setup\nSoftware Requirements\nTroubleshooting\nCitations\nLicense",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/index.html#quick-start",
    "href": "content/index.html#quick-start",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "If youâ€™re eager to run the pipeline quickly, jump to:\nğŸ‘‰ Quick Start Guide\nOr explore the Docker options for portable setup:\nğŸ‘‰ Docker Setup",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/license.html",
    "href": "content/license.html",
    "title": "ğŸ“„ MIT License",
    "section": "",
    "text": "1 MIT License\nCopyright (c) 2025 Nancy Anderson\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the â€œSoftwareâ€), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "crumbs": [
      "ğŸ“„ MIT License"
    ]
  },
  {
    "objectID": "content/pipeline1.html#supported-data-types",
    "href": "content/pipeline1.html#supported-data-types",
    "title": "ğŸ”„ Pipeline 1: QC & Preprocessing Modules",
    "section": "2 ğŸ§¬ Supported Data Types",
    "text": "2 ğŸ§¬ Supported Data Types\nThis pipeline is designed for ChIP-seq and ATAC-seq data preprocessing. It includes:\n\nAdapter trimming (cutadapt)\nParallel quality control (FastQC)\nOptional spike-in filtering\nMitochondrial read removal\nBlacklist region filtering (ENCODE)\nClean alignment with BWA and BAM post-processing\n\nBy supporting these steps, the pipeline ensures high-quality, reproducible inputs for downstream peak calling or chromatin accessibility analysis.\n\nğŸ” While optimized for ChIP-seq, the pipeline is fully compatible with ATAC-seq experiments.\n\nScripts: 01_init_directories.sh to 11_Renaming_bam.sh\nmodules/pipeline1\\$ tree . \nâ”œâ”€â”€ 01_init_directories.sh \nâ”œâ”€â”€ 02_reference_check.sh \nâ”œâ”€â”€ 03_input_fetch.sh \nâ”œâ”€â”€ 04_fastqc_parallel.sh \nâ”œâ”€â”€ 05_05_Cutadapt_trimming_phix_parallel.sh \nâ”œâ”€â”€ 06_fastqc_trimmed_parallel.sh \nâ”œâ”€â”€ 07_spike_detect.sh \nâ”œâ”€â”€ 08_alignment_bwa_spike.sh \nâ”œâ”€â”€ 09_readgroups_add.sh \nâ”œâ”€â”€ 10_bam_cleaning.sh \nâ”œâ”€â”€ 10_plot_spike_qc_summary.R \nâ””â”€â”€ 11_Renaming_bam.sh\nGreat structure! Given the flow of your Quarto document, the best place to add the usage documentation for run_pipeline1.sh would be after the ## ğŸ§¬ Supported Data Types section and before the ### ğŸ”§ Modules section.\nThis keeps your document logically organized:\n\nIntro\nWhat data it supports\nHow to run it (Usage) âœ…\nWhat each module does\nWhat the outputs are\n\n\n\n2.1 âœ… Suggested Update (add this block in your .qmd file):\n### ğŸš€ Usage: `run_pipeline1.sh`\n\nThis script is the entry point for Pipeline 1 and supports both default and customized executions.\n\n#### âœ… Default Run\n\nRun with all defaults:\n```bash\nbash run_pipeline1.sh\nThis uses:\n\nTHREADS = 4\nREFERENCE = hg38\nADAPTER = tn5_truseq\nPLATFORM = ILLUMINA\nMAPPING = metadata/mapping.tsv\nREPORT_FORMAT = human\n\n\n2.1.1 ğŸ”§ Full Custom Run\nExample of overriding all defaults:\nbash run_pipeline1.sh -t 8 -r mm10 -a tn5_nextera -p IONTORRENT -m data/sample_metadata.tsv -f csv\nOverrides:\n\n-t 8: 8 threads\n-r mm10: mouse genome\n-a tn5_nextera: alternative adapter\n-p IONTORRENT: sequencing platform\n-m data/sample_metadata.tsv: custom mapping file\n-f csv: report format\n\n\n\n2.1.2 â„¹ï¸ Help\nFor option details:\nbash run_pipeline1.sh --help\n\n\n\n2.2 ğŸ”§ Modules\n\nğŸ”§ Modules Overview\n\n\nScript\nPurpose\n\n\n\n\n01_init_directories.sh\nCreates necessary directory structure.\n\n\n02_reference_check.sh\nValidates presence of genome reference files.\n\n\n03_input_fetch.sh\nFetches and decompresses user-supplied FASTQ files.\n\n\n04_fastqc_parallel.sh\nRuns FastQC and generates quality reports.\n\n\n05_Cutadapt_trimming_phix_parallel.sh\nTrims adapters and low-quality bases using Cutadapt.\n\n\n06_fastqc_trimmed_parallel.sh\nRuns FastQC on trimmed reads.\n\n\n07_spike_detect.sh\nDetects exogenous spike-in if not provided.\n\n\n08_alignment_bwa_spike.sh\nAligns reads using BWA and checks spike content.\n\n\n09_readgroups_add.sh\nAdds read group metadata using Picard.\n\n\n10_bam_cleaning.sh\nCleans and indexes BAMs; performs spike-in QC.\n\n\n11_Renaming_bam.sh\nRenames BAMs using mapping.tsv for grouping.\n\n\n\n\n\n2.3 ğŸ“ Output Folders\nresults/: Raw outputs and intermediate files.\nanalysis/Renamed_Cleaned: Cleaned and filtered BAMs for downstream usage.\nresults/\nresults/Filtered$ tree \n\nâ”œâ”€â”€ BAM}\nâ”œâ”€â”€ Filtered\nâ”œ        â”œâ”€â”€ Cleaned  âœ… here are the cleaned BAM that will be renamed. â”‚\nâ”œ        â”œâ”€â”€ Deduplicated\nâ”œ        â””â”€â”€ Metrics\nâ”œâ”€â”€ QC_fastqc\nâ”œâ”€â”€ QC_spike_plots\nâ”œâ”€â”€ QC_trimmed_fastqc\nâ”œâ”€â”€ spike_analysis\nâ””â”€â”€ Trimmed",
    "crumbs": [
      "ğŸ”„ Pipeline 1: QC & Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/reference_setup.html",
    "href": "content/reference_setup.html",
    "title": "ğŸ§¬ Initial documents and reference setups",
    "section": "",
    "text": "The following helper scripts are available in the modules/utils/ directory for setup and validation:\n\n\nAutomates the preparation of genome reference directories and indexes.\nUsage:\nbash modules/utils/setup_reference.sh &lt;GENOME_NAME&gt; &lt;GENOME_FASTA&gt;\nExample:\nbash modules/utils/setup_reference.sh hg38 /path/to/hg38.fa\nWhat it does:\n\nCreates a genome folder under resources/genomes/GENOME_NAME/\nGenerates:\n\n.fai index via samtools faidx\nbwa index (bwa index)\nChromosome sizes (.chrom.sizes)\nDictionary via picard CreateSequenceDictionary\n\n\n\n\n\nPrepares spike-in reference genomes similarly to the main genome.\nUsage:\nbash modules/utils/setup_spikein_refs.sh &lt;SPIKE_NAME&gt; &lt;SPIKE_FASTA&gt;\nExample:\nbash modules/utils/setup_spikein_refs.sh dm6 /path/to/dm6.fa\nCreates:\n\nIndexed bwa genome and dictionary for the spike\nOrganized under resources/spikein/SPIKE_NAME/\n\nThese scripts must be run before alignment steps in the pipeline to ensure all genome/spike references are available and indexed properly.\nLet me know if youâ€™d like a Makefile target to wrap these commands for common genomes (e.g., hg38 + dm6).",
    "crumbs": [
      "ğŸ§¬ Initial documents and reference setups"
    ]
  },
  {
    "objectID": "content/reference_setup.html#utilities",
    "href": "content/reference_setup.html#utilities",
    "title": "ğŸ§¬ Initial documents and reference setups",
    "section": "",
    "text": "The following helper scripts are available in the modules/utils/ directory for setup and validation:\n\n\nAutomates the preparation of genome reference directories and indexes.\nUsage:\nbash modules/utils/setup_reference.sh &lt;GENOME_NAME&gt; &lt;GENOME_FASTA&gt;\nExample:\nbash modules/utils/setup_reference.sh hg38 /path/to/hg38.fa\nWhat it does:\n\nCreates a genome folder under resources/genomes/GENOME_NAME/\nGenerates:\n\n.fai index via samtools faidx\nbwa index (bwa index)\nChromosome sizes (.chrom.sizes)\nDictionary via picard CreateSequenceDictionary\n\n\n\n\n\nPrepares spike-in reference genomes similarly to the main genome.\nUsage:\nbash modules/utils/setup_spikein_refs.sh &lt;SPIKE_NAME&gt; &lt;SPIKE_FASTA&gt;\nExample:\nbash modules/utils/setup_spikein_refs.sh dm6 /path/to/dm6.fa\nCreates:\n\nIndexed bwa genome and dictionary for the spike\nOrganized under resources/spikein/SPIKE_NAME/\n\nThese scripts must be run before alignment steps in the pipeline to ensure all genome/spike references are available and indexed properly.\nLet me know if youâ€™d like a Makefile target to wrap these commands for common genomes (e.g., hg38 + dm6).",
    "crumbs": [
      "ğŸ§¬ Initial documents and reference setups"
    ]
  },
  {
    "objectID": "content/pipeline2.html",
    "href": "content/pipeline2.html",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "",
    "text": "Pipeline2 continues the analysis after preprocessing and alignment, performing replicate QC, peak calling, and reproducibility evaluation via IDR. It supports both MACS3 and HOMER peak callers, with outputs from both individual and pooled replicates.\nDuring the IDR evaluation, a series of QC analyses is performed, and an HTML/PDF report is generated and stored at:\nThe report includes and are in HTML and pdf format:",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#functional-enrichment-report",
    "href": "content/pipeline2.html#functional-enrichment-report",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "1 ğŸ§¬ Functional Enrichment Report",
    "text": "1 ğŸ§¬ Functional Enrichment Report\nThe final enrichment summary report (including Gene Ontology, KEGG, Reactome, and g:Profiler figures) is available as a downloadable PDF and HTML. If the PRE_POSSIBLE_OUTCOME.tsv file is too large, the PDF will include only the g:Profiler figures. The full PRE_POSSIBLE_OUTCOME.tsv content will remain available in its original TSV format and within the HTML report.\n\n1.1 ğŸ“ Example Reports\nğŸ‘‰ hg38: ğŸ“ View report (PDF)\nğŸ‘‰ mm10: ğŸ“ View report (PDF)\n\n\n\n1.2 ğŸš€ Usage: run_pipeline2.sh\nThis script runs Pipeline 2 â€” performing peak calling, replicate QC, IDR analysis, and functional annotation using MACS3 and HOMER.\n\n1.2.1 âœ… Default Run\nRun with all default parameters:\nbash run_pipeline2.sh\nThis will use:\n\nREFERENCE = hg38\nMAPPING = metadata/mapping.tsv\nPEAK_CALLER = both (MACS3 and HOMER)\nTHREADS = 4\nIDR_THRESHOLD = 0.05\nTARGET_RULES = templates/target_peak_rules.yaml\nTSS_DB = HOMER (for annotation)\n\n\nâœ… Defaults are suitable for most ChIP-seq and ATAC-seq use cases.\n\n\n\n\n1.2.2 ğŸ”§ Full Custom Run\nCustomize all parameters:\nbash run_pipeline2.sh \\\n  -r mm10 \\\n  -m data/sample_metadata.tsv \\\n  -p macs3 \\\n  -t 8 \\\n  -i 0.01 \\\n  -y templates/target_peak_rules_HOMER.yaml \\\n  --tss-db clusterProfiler\nExplanation:\n\n-r mm10 â†’ Use mouse genome\n-m data/sample_metadata.tsv â†’ Custom metadata file\n-p macs3 â†’ Use MACS3 only (can be macs3, homer, or both)\n-t 8 â†’ Use 8 threads\n-i 0.01 â†’ Adjust IDR threshold\n-y templates/target_peak_rules_HOMER.yaml â†’ Custom target rules\n--tss-db clusterProfiler â†’ Use clusterProfiler instead of HOMER for gene annotation.\n\n\n\n\n\n\n\n#### â„¹ï¸ Help\n\n\n\n\n### ğŸ“„ 1. mapping_schema.yaml â€” Metadata Validation Rules\n\n\nDefines structure and validation logic for your mapping.tsv metadata.\n\n\n#### ğŸ” Key Elements:\n\n\n- Required fields: Sample_ID, Instrument, Sample_Type, Condition, Replicate, Target - Optional fields: Cell_line, Spike_Type - Regex validation: For fields like Sample_Type, Replicate\n\n\n#### âœ… Validate Schema File\n\n\nbash bash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml #### âœ… Validate Metadata File\n\n\nbash python3 modules/utils/validate_mapping.py \\ --mapping metadata/mapping.tsv \\ --schema templates/mapping_schema.yaml\n\n\n\n\n\n\n1.3 ğŸ“„ 2. target_peak_rules.yaml â€” Peak Calling Styles (MACS3)\nThis config determines which targets require narrow or broad peak calling.\nExamples:\n\nCTCF, MYC â†’ narrowPeak\nH3K27me3, EZH2 â†’ broadPeak\n\n\n\n\n1.4 ğŸ“„ 3. target_peak_rules_HOMER.yaml â€” Peak Calling Styles (HOMER)\nClassifies targets by style for findPeaks:\n\nfactor â†’ e.g., MYC, SOX2\nhistone â†’ e.g., H3K27me3, EZH2\nGROseq, DNaseI, etc.\n\n\nâš ï¸ Note: These files are pre-validated. If you add new targets, you must write your own validation.",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "href": "content/pipeline2.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh",
    "text": "2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh\nmodules/pipeline2/\nâ”œâ”€â”€ 01_replicate_qc.sh\nâ”œâ”€â”€ 02_merge_pseudoreplicates.sh\nâ”œâ”€â”€ 03_1_MACS3_peak_calling.sh\nâ”œâ”€â”€ 03_2_homer_peak_calling_fold_fdr_relaxed.sh\nâ”œâ”€â”€ 03_2_homer_to_peakFormat.R\nâ”œâ”€â”€ 03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 03_4_homer_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 04_plot_idr_summary.R\nâ”œâ”€â”€ 04_run_idr.sh\nâ”œâ”€â”€ 05_peak_annotation.sh\nâ”œâ”€â”€ 06_igv_snapshot.sh\nâ””â”€â”€ cluster_enrichment_updated_hg_mice.R\n\n2.1 ğŸ”§ Modules\n\n\n\n\n\n\n\nScript\nPurpose\n\n\n\n\n01_replicate_qc.sh\nAnalyzes biological replicates for correlation (cutoff: 0.8), filters low-correlation samples.\n\n\n02_merge_pseudoreplicates.sh\nMerges BAMs and creates pseudo-replicates for IDR.\n\n\n03_1_MACS3_peak_calling.sh\nPeak calling using MACS3 with smart baseline control matching.\n\n\n03_2_homer_peak_calling_fold_fdr_relaxed.sh\nHOMER-based peak calling using target_peak_rules_HOMER.yaml.\n\n\n03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nMACS3 peak calling for pooled/pseudo BAMs.\n\n\n03_4_homer_peak_calling_pooled_pseudoreps.sh\nHOMER peak calling for pooled/pseudo BAMs.\n\n\n04_run_idr.sh\nRuns IDR on replicate pairs, pooled vs individual, etc.\n\n\n05_peak_annotation.sh\nPerforms TSS annotation of peak files.\n\n\n06_igv_snapshot.sh\nGenerates IGV snapshots for peak visualization.\n\n\n\n\n\n\n\n\nâš ï¸ Note :06_igv_snapshot.sh is an extra module not yet adapted to the pipeline. This particular script produce to many snapshots if the user do not define the peaks based on their needs. â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#analysis-folder-structure",
    "href": "content/pipeline2.html#analysis-folder-structure",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "3 ğŸ—‚ Analysis Folder Structure",
    "text": "3 ğŸ—‚ Analysis Folder Structure\nanalysis$ tree -L 1\n.\nâ”œâ”€â”€ BAM_replicate_fail\nâ”œâ”€â”€ ChIPseeker_TSS_Hommer_IDR_annotation\nâ”œâ”€â”€ IDR_Results\nâ”‚   â”œâ”€â”€ homer\nâ”‚   â””â”€â”€ macs3\nâ”œâ”€â”€ PeakCalling_HOMER\nâ”œâ”€â”€ PeakCalling_HOMER_pool_pseudo\nâ”œâ”€â”€ PeakCalling_MACS3\nâ”œâ”€â”€ PeakCalling_MACS3_pool_pseudo\nâ”œâ”€â”€ Pooled_BAMs\nâ”œâ”€â”€ pooling_log.tsv\nâ”œâ”€â”€ Pool_Pseudo_QC_stats\nâ”œâ”€â”€ Pseudoreplicates\nâ”œâ”€â”€ Renamed_Cleaned\nâ””â”€â”€ Replicate_QC\n    â”œâ”€â”€ bigwig\n    â”œâ”€â”€ deeptools\n    â”œâ”€â”€ pbc_metrics.tsv\n    â””â”€â”€ tmp_groups",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/docker_guide.html",
    "href": "content/docker_guide.html",
    "title": "ğŸ³ Docker guide",
    "section": "",
    "text": "Docker provides a reproducible, isolated environment for running the pipeline. Once the image is built, the entire analysis can be executed without requiring local software installations.\n\n0.1 âš™ï¸ Running the Pipeline in Docker\nUse the following command to launch the container:\ndocker run -v \"$PWD:/pipeline2\" -w /pipeline2 pipeline2-image \\\n  bash run_pipeline2.sh [options]\nThis command:\n\nMounts your current directory ($PWD) into the container at /pipeline2\nSets /pipeline2 as the working directory\nExecutes the pipeline script inside the container\n\n\n\n0.2 ğŸ“ Project Structure Inside the Container\nYour pipeline files are accessible under:\n/pipeline2/\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/pipeline2/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ logs/\nâ”œâ”€â”€ analysis/\nRelative paths like modules/pipeline2/04_run_idr.sh will resolve correctly.\n\n\n\n0.3 ğŸ”— Docker Command Summary\n\n\n\n\n\n\n\nStep\nCommand\n\n\n\n\nBuild image\ndocker build -f Dockerfile.pipeline2 -t pipeline2-image .\n\n\nRun pipeline\ndocker run -v \"$PWD:/pipeline2\" -w /pipeline2 pipeline2-image ...\n\n\nView results\nCheck output files in your local analysis/ and logs/ directories\n\n\n\n\nFor advanced options or troubleshooting, refer to the main documentation or contact the maintainers.",
    "crumbs": [
      "ğŸ³ Docker guide"
    ]
  },
  {
    "objectID": "content/Conda.html",
    "href": "content/Conda.html",
    "title": "ğŸ Core enviroment:conda",
    "section": "",
    "text": "Below are two separate Conda environment YAML files:\nThis includes all necessary packages for running the pipeline in a non-Docker setup, excluding R packages and HOMER.\nname: chipseq_pipeline_base\nchannels:\n  - bioconda\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.9\n  - pip\n  - numpy\n  - pandas\n  - pyyaml\n  - samtools&gt;=1.14\n  - bedtools&gt;=2.30.0\n  - bwa\n  - cutadapt&gt;=4.0\n  - macs3\n  - deeptools\n  - jq\n  - r-base=4.2\n  - r-data.table\n  - r-ggplot2\n  - r-dplyr\n  - r-readr\n  - r-tidyr\n  - r-stringr\n  - r-jsonlite\n  - r-httr\n  - r-remotes  # for installing from GitHub\n  - pip:\n      - idr  # we patch it manually later",
    "crumbs": [
      "ğŸ Core enviroment:conda"
    ]
  },
  {
    "objectID": "content/Conda.html#software-requirements-non-docker-setup",
    "href": "content/Conda.html#software-requirements-non-docker-setup",
    "title": "ğŸ Core enviroment:conda",
    "section": "1 âš™ï¸ Software Requirements (Non-Docker Setup)",
    "text": "1 âš™ï¸ Software Requirements (Non-Docker Setup)\nWhile this pipeline is fully Docker-compatible, you can also run it natively by installing the required software through ğŸ Conda and ğŸ“˜ â„ .\n\n1.1 ğŸ“ Environment Files\nAll environment setup files are located in:\nenv/\nâ”œâ”€â”€ env_pipeline_base.yml     # Conda YAML file (base system + CLI tools)\nâ”œâ”€â”€ env_pipeline_r.R          # R script to install required R/Bioconductor packages\n\n\n1.2 ğŸ§ª Step 1: Create Conda Environment\nUse the provided YAML file to create a clean Conda environment with all the command-line tools.\nconda env create -f env/env_pipeline_base.yml\nconda activate chipseq_pipeline_base\n\n\n1.3 ğŸ“¦ Step 2: Install R and Bioconductor Packages\nOnce inside the conda environment, install the required R packages using:\nRscript env/env_pipeline_r.R\nThis script will:\n\nCheck for missing CRAN or Bioconductor dependencies\nInstall them as needed\nLoad the correct annotation packages for both human (hg38) and mouse (mm10) genomes\n\n\n\n1.4 ğŸ§° Tool Dependencies\nSome tools like picard.jar and qualimap are not installed via Conda but are required. These are bundled inside the pipeline under:\ntools/\nâ”œâ”€â”€ picard.jar      # Required for BAM metadata tagging\nâ”œâ”€â”€ qualimap/       # For BAM quality metrics (must be Java-compatible)\nMake sure:\n\ntools/picard.jar is accessible (used by AddOrReplaceReadGroups)\ntools/qualimap/ is executable and on your PATH or referenced directly by the scripts\n\nEnsure these are available in your environment or adjust the relevant paths in the pipeline scripts.\nğŸ³ Or Use Docker For reference, installation steps are also reflected in the Dockerfiles:\nbash Copy Edit Dockerfile.pipeline1 Dockerfile.pipeline2 These show the exact installation process and can be adapted for manual setup if needed.",
    "crumbs": [
      "ğŸ Core enviroment:conda"
    ]
  }
]