[
  {
    "objectID": "Built and mount docker inside thepackage.html",
    "href": "Built and mount docker inside thepackage.html",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "cd chip-seq-modular-pipeline docker build -f Docker/Dockerfile.pipeline1 -t pipeline1-test .\nâœ… 1. Docker Command with Defaults Only\ndocker run â€“rm -it\n-v â€œ\\(PWD/samples:/pipeline/samples\" \\\n  -v \"\\)PWD/Reference:/pipeline/Referenceâ€\n-v â€œ\\(PWD/SpikeinReference:/pipeline/SpikeinReference\" \\\n  -v \"\\)PWD/metadata:/pipeline/metadataâ€\n-v â€œ\\(PWD/templates:/pipeline/templates\" \\\n  -v \"\\)PWD/adapters:/pipeline/adaptersâ€\n-v â€œ\\(PWD/assets:/pipeline/assets\" \\\n  -v \"\\)PWD/modules:/pipeline/modulesâ€\n-v â€œ\\(PWD/results:/pipeline/results\" \\\n  -v \"\\)PWD/analysis:/pipeline/analysisâ€\npipeline1-test bash run_pipeline1.sh This runs the pipeline with:\nTHREADS=4\nREFERENCE=hg38\nADAPTER=tn5_truseq\nPLATFORM=ILLUMINA\nMAPPING=metadata/mapping.tsv\nREPORT_FORMAT=human\nâš™ï¸ 2. Docker Command with All Options Provided docker run â€“rm -it\n-v â€œ\\(PWD/samples:/pipeline/samples\" \\\n  -v \"\\)PWD/Reference:/pipeline/Referenceâ€\n-v â€œ\\(PWD/SpikeinReference:/pipeline/SpikeinReference\" \\\n  -v \"\\)PWD/metadata:/pipeline/metadataâ€\n-v â€œ\\(PWD/templates:/pipeline/templates\" \\\n  -v \"\\)PWD/adapters:/pipeline/adaptersâ€\n-v â€œ\\(PWD/assets:/pipeline/assets\" \\\n  -v \"\\)PWD/modules:/pipeline/modulesâ€\n-v â€œ\\(PWD/results:/pipeline/results\" \\\n  -v \"\\)PWD/analysis:/pipeline/analysisâ€\npipeline1-test bash run_pipeline1.sh\n-t 8\n-r mm10\n-a tn5_nextera\n-p IONTORRENT\n-m metadata/mapping.tsv\n-f csv"
  },
  {
    "objectID": "content/troubleshooting.html",
    "href": "content/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "â— Troubleshooting / FAQâ€\n\nğŸ” Metadata Validation Fails\nğŸ›‘ IDR Errors: NumPy or Missing Peaks\nâš ï¸ Docker Mount Issues Troubleshooting & FAQ **\nğŸ” Metadata Validation Fails\nğŸ›‘ IDR Errors: NumPy or Missing Peaks 4.3 âš ï¸ Docker Mount Issues",
    "crumbs": [
      "Troubleshooting"
    ]
  },
  {
    "objectID": "content/software.html",
    "href": "content/software.html",
    "title": "ğŸ§° Software Requirements (Non-Docker Setup)",
    "section": "",
    "text": "If youâ€™re running the pipeline without Docker, your system must have the following tools installed and available in your PATH â€” except for picard.jar and qualimap, which are already bundled in the pipeline under tools/ and should remain there.\n\n\nInstall the following tools via your package manager (apt, brew, conda, etc.) or from source as needed:\n\n\n\n\n\n\n\n\nTool\nMinimum Version\nPurpose\n\n\n\n\nbash\n4.0+\nShell scripting\n\n\ncutadapt\n4.0+\nAdapter trimming\n\n\nbwa\n0.7.17+\nRead alignment\n\n\nsamtools\n1.14+\nBAM/SAM manipulation\n\n\npicard\n2.26.10+\nBAM post-processing\n\n\nmacs3\n3.0.0a6+\nPeak calling\n\n\nhomer\nv4.11+\nAlternative peak calling\n\n\npython3\n3.9+\nUsed for helper scripts\n\n\npip\nLatest\nPython package management\n\n\nR\n4.2+\nStatistical computing environment\n\n\nRscript\n4.2+\nScript execution for R modules\n\n\nbedtools\n2.30.0+\nGenomic interval operations\n\n\ndeepTools\n3.5.1+\nQC and coverage tools\n\n\nidr\n2.0.4.2 (patched)\nPeak reproducibility scoring\n\n\njq\n1.6+\nJSON parsing (metadata scripts)\n\n\nwget/curl\nany\nData downloading\n\n\nyq\n4.0+\nYAML parsing and editing (Go-based)\n\n\n\n\n\n\nInstall with:\npip install numpy pandas pyyaml\n\n\n\nYour pipeline includes scripts that require the following CRAN and Bioconductor packages:\n\n\n\ndplyr\nggplot2\ntidyr\ndata.table\nreadr\nstringr\ntools\njsonlite\nhttr\n\n\n\n\n\nclusterProfiler\nenrichplot\nReactomePA\nreactome.db\nbiomaRt\nAnnotationDbi\norg.Hs.eg.db\norg.Mm.eg.db\n\n\nâœ… The R scripts automatically attempt to install missing packages if your internet connection is available.\n\n\n\n\n\n\nFastQC\nMultiQC\nIGV (for manual visualization)\ndocker (if youâ€™d like to use pre-built containers)",
    "crumbs": [
      "ğŸ§° Software Requirements (Non-Docker Setup)"
    ]
  },
  {
    "objectID": "content/software.html#software-requirements-non-docker-setup",
    "href": "content/software.html#software-requirements-non-docker-setup",
    "title": "ğŸ§° Software Requirements (Non-Docker Setup)",
    "section": "",
    "text": "If youâ€™re running the pipeline without Docker, your system must have the following tools installed and available in your PATH â€” except for picard.jar and qualimap, which are already bundled in the pipeline under tools/ and should remain there.\n\n\nInstall the following tools via your package manager (apt, brew, conda, etc.) or from source as needed:\n\n\n\n\n\n\n\n\nTool\nMinimum Version\nPurpose\n\n\n\n\nbash\n4.0+\nShell scripting\n\n\ncutadapt\n4.0+\nAdapter trimming\n\n\nbwa\n0.7.17+\nRead alignment\n\n\nsamtools\n1.14+\nBAM/SAM manipulation\n\n\npicard\n2.26.10+\nBAM post-processing\n\n\nmacs3\n3.0.0a6+\nPeak calling\n\n\nhomer\nv4.11+\nAlternative peak calling\n\n\npython3\n3.9+\nUsed for helper scripts\n\n\npip\nLatest\nPython package management\n\n\nR\n4.2+\nStatistical computing environment\n\n\nRscript\n4.2+\nScript execution for R modules\n\n\nbedtools\n2.30.0+\nGenomic interval operations\n\n\ndeepTools\n3.5.1+\nQC and coverage tools\n\n\nidr\n2.0.4.2 (patched)\nPeak reproducibility scoring\n\n\njq\n1.6+\nJSON parsing (metadata scripts)\n\n\nwget/curl\nany\nData downloading\n\n\nyq\n4.0+\nYAML parsing and editing (Go-based)\n\n\n\n\n\n\nInstall with:\npip install numpy pandas pyyaml\n\n\n\nYour pipeline includes scripts that require the following CRAN and Bioconductor packages:\n\n\n\ndplyr\nggplot2\ntidyr\ndata.table\nreadr\nstringr\ntools\njsonlite\nhttr\n\n\n\n\n\nclusterProfiler\nenrichplot\nReactomePA\nreactome.db\nbiomaRt\nAnnotationDbi\norg.Hs.eg.db\norg.Mm.eg.db\n\n\nâœ… The R scripts automatically attempt to install missing packages if your internet connection is available.\n\n\n\n\n\n\nFastQC\nMultiQC\nIGV (for manual visualization)\ndocker (if youâ€™d like to use pre-built containers)",
    "crumbs": [
      "ğŸ§° Software Requirements (Non-Docker Setup)"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html",
    "href": "content/docker_pipeline2.html",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "",
    "text": "Pipeline2 is the second part of the ChIP-seq modular system. It performs:\n\nPeak calling (MACS3 or HOMER)\nReproducibility analysis (IDR)\nAnnotation and final reporting\n\nIt is designed to consume output from Pipeline1 (analysis/) and produce final results in results/.",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#what-is-pipeline2",
    "href": "content/docker_pipeline2.html#what-is-pipeline2",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "",
    "text": "Pipeline2 is the second part of the ChIP-seq modular system. It performs:\n\nPeak calling (MACS3 or HOMER)\nReproducibility analysis (IDR)\nAnnotation and final reporting\n\nIt is designed to consume output from Pipeline1 (analysis/) and produce final results in results/.",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#requirements",
    "href": "content/docker_pipeline2.html#requirements",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "2 âœ… Requirements",
    "text": "2 âœ… Requirements\n\nDocker installed (docker --version)\nPipeline image built (see below)\nDirectory structure including:\n\n\nReference/         # Genome references\nmetadata/          # Sample and group mapping\ntemplates/         # Configuration files\nanalysis/          # Output from pipeline1\nresults/           # New output\ntools/, assets/    # Utility scripts and helper data\nmodules/           # Contains pipeline2 logic\n\n\n\n2.1 ğŸ“ 2. Project Structure\nA typical folder layout:\nyour_project/\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/\nâ”‚   â””â”€â”€ pipeline2/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ templates/\nâ”œâ”€â”€ logs/\nâ”œâ”€â”€ analysis/           # Will be populated by the pipeline\nEnsure all these folders exist, especially Reference/, metadata/, and modules/.",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#section",
    "href": "content/docker_pipeline2.html#section",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "3 â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”",
    "text": "3 â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#build-the-docker-image",
    "href": "content/docker_pipeline2.html#build-the-docker-image",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "4 ğŸ—ï¸ Build the Docker Image",
    "text": "4 ğŸ—ï¸ Build the Docker Image\nFrom your project root:\ndocker build -f Docker/Dockerfile.pipeline2 -t pipeline2-test .\nThis command builds a Docker image using your latest Dockerfile.pipeline2.",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#folder-mounts-summary",
    "href": "content/docker_pipeline2.html#folder-mounts-summary",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "5 ğŸ“ Folder Mounts Summary",
    "text": "5 ğŸ“ Folder Mounts Summary\n\n\n\nHost Folder\nContainer Path\nPurpose\n\n\n\n\nanalysis/\n/data/analysis\nInput from pipeline1\n\n\nresults/\n/data/results\nOutput from pipeline2\n\n\nReference/\n/data/Reference\nGenome references\n\n\nmetadata/\n/data/metadata\nMetadata and mapping\n\n\ntemplates/\n/data/templates\nPipeline configuration\n\n\nmodules/\n/data/modules\nContains pipeline2 logic\n\n\nassets/\n/data/assets\nVisual assets and icons\n\n\ntools/\n/pipeline/tools\nCustom scripts and binaries",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#run-with-default-options",
    "href": "content/docker_pipeline2.html#run-with-default-options",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "6 â–¶ï¸ Run With Default Options",
    "text": "6 â–¶ï¸ Run With Default Options\ndocker run --rm -it \\\n  -v \"$PWD/analysis:/data/analysis\" \\\n  -v \"$PWD/results:/data/results\" \\\n  -v \"$PWD/Reference:/data/Reference\" \\\n  -v \"$PWD/metadata:/data/metadata\" \\\n  -v \"$PWD/templates:/data/templates\" \\\n  -v \"$PWD/modules:/data/modules\" \\\n  -v \"$PWD/assets:/data/assets\" \\\n  -v \"$PWD/tools:/pipeline/tools\" \\\n  pipeline2-test \\\n  bash run_pipeline2.sh --caller homer --genome hg38",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#full-example-with-options",
    "href": "content/docker_pipeline2.html#full-example-with-options",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "7 âš™ï¸ Full Example With Options",
    "text": "7 âš™ï¸ Full Example With Options\ndocker run --rm -it \\\n  -v \"$PWD/analysis:/data/analysis\" \\\n  -v \"$PWD/results:/data/results\" \\\n  -v \"$PWD/Reference:/data/Reference\" \\\n  -v \"$PWD/metadata:/data/metadata\" \\\n  -v \"$PWD/templates:/data/templates\" \\\n  -v \"$PWD/modules:/data/modules\" \\\n  -v \"$PWD/assets:/data/assets\" \\\n  -v \"$PWD/tools:/pipeline/tools\" \\\n  pipeline2-test \\\n  bash run_pipeline2.sh \\\n    --caller macs3 \\\n    --genome mm10 \\\n    -t 8 \\\n    -m metadata/mapping.tsv \\\n    -f true \\\n    -c spearman\n\nâœ… --caller: choose macs3 or homer âœ… -t: number of threads âœ… -f: filtering enabled (true/false) âœ… -c: correlation method",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#dry-run-mode",
    "href": "content/docker_pipeline2.html#dry-run-mode",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "8 ğŸ” Dry Run Mode",
    "text": "8 ğŸ” Dry Run Mode\nIf your pipeline supports --dry-run, test like this:\nbash run_pipeline2.sh --caller homer --dry-run\nInside Docker:\ndocker run --rm -it pipeline2-test\ncd /pipeline && bash run_pipeline2.sh --dry-run",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#output",
    "href": "content/docker_pipeline2.html#output",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "9 ğŸ“‚ Output",
    "text": "9 ğŸ“‚ Output\n\n\n\nFolder\nDescription\n\n\n\n\nresults/\nFinal peak files, reports, plots\n\n\nanalysis/\nUsed from pipeline1 as source input\n\n\nlogs/\nPer-module logs (optional, if implemented)",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#troubleshooting",
    "href": "content/docker_pipeline2.html#troubleshooting",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "10 ğŸ§¼ Troubleshooting",
    "text": "10 ğŸ§¼ Troubleshooting\n\nMake sure your Docker mounts point to actual directories (ls to verify).\nTry running Docker without volumes first to debug logic:\ndocker run --rm -it pipeline2-test\nbash run_pipeline2.sh --caller homer",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline2.html#next-steps",
    "href": "content/docker_pipeline2.html#next-steps",
    "title": "ğŸ³ Running Pipeline2 with Docker",
    "section": "11 âœ… Next Steps",
    "text": "11 âœ… Next Steps\nYou can now:\n\nValidate outputs in results/\nView filtered metadata in metadata/mapping_filtered.tsv\nOptionally generate a downstream report or visualizations\n\n\n\n11.1 ğŸ› ï¸ï¸ï¸ 6. Optional: Helper Shell Script\nCreate a script like run.sh:\n#!/bin/bash\ndocker run --rm -it \\\n  -v \"$PWD/analysis:/data/analysis\" \\\n  -v \"$PWD/results:/data/results\" \\\n  -v \"$PWD/Reference:/data/Reference\" \\\n  -v \"$PWD/metadata:/data/metadata\" \\\n  -v \"$PWD/templates:/data/templates\" \\\n  -v \"$PWD/modules:/data/modules\" \\\n  -v \"$PWD/assets:/data/assets\" \\\n  -v \"$PWD/tools:/pipeline/tools\" \\\n  pipeline2-test \\\n  bash run_pipeline2.sh \"$@\"\nThen run it like this:\nchmod +x run.sh\n./run.sh --caller homer -genome mm10",
    "crumbs": [
      "ğŸ³ Running Pipeline2 with Docker"
    ]
  },
  {
    "objectID": "content/citations.html",
    "href": "content/citations.html",
    "title": "ğŸ“š Tool Citations",
    "section": "",
    "text": "We respectfully acknowledge the following software used in this pipeline:\n\nCutadapt â€” Martin (2011) DOI: 10.14806/ej.17.1.200\n\nBWA â€” Li and Durbin (2009) PMID: 19451168\n\nSAMtools â€” Li et al.Â (2009) PMID: 19505943\n\nMACS3 â€” Zhang et al.Â (2008) PMID: 18798982\n\nHOMER â€” Heinz et al.Â (2010) PMID: 20513432\n\nPicard â€” Broad Institute https://broadinstitute.github.io/picard/\n\ndeepTools â€” RamÃ­rez et al.Â (2016) PMID: 27079975\n\nQualimap â€” Okonechnikov et al.Â (2016) PMID: 27040156\n\nIDR â€” Li et al.Â (2011) PMID: 21300883\n\nclusterProfiler â€” Yu et al.Â (2012) PMID: 22455463\n\nReactomePA â€” Yu and He (2016) PMID: 27480138\n\nyq â€” Mike Farah (v4+), GitHub: https://github.com/mikefarah/yq\n\nAnd others listed in the software requirements section above.\n\nâš ï¸ For academic use, please cite the original tools if you rely on results from this pipeline.",
    "crumbs": [
      "ğŸ“š Tool Citations"
    ]
  },
  {
    "objectID": "content/mapping.html",
    "href": "content/mapping.html",
    "title": "Preprocessing Modules",
    "section": "",
    "text": "The metadata file is crucial for automating sample grouping, identifying replicates, and selecting the appropriate peak caller.\n\nğŸ§  Filename required: metadata/mapping.tsv\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nSample_ID\nUnique ID matching FASTQ or BAM files\nSRR123456\n\n\nInstrument\nSequencing instrument or platform\nIllumina\n\n\nSample_Type\nChIP, Input, IgG, Mock, etc.\nChIP\n\n\nCondition\nExperimental condition or group\nTreated, WT\n\n\nReplicate\nReplicate number (1, 2, 3, â€¦)\n1\n\n\nTarget\nTranscription factor or histone mark\nH3K27ac, CTCF\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nCell_line\nCell line used\nHEK293\n\n\nSpike_Type\nIf spike-in is used (e.g., dm6, ERCC)\ndm6\n\n\n\n\n\n\nSample_ID   Instrument  Sample_Type Condition   Replicate   Target  Cell_line   Spike_Type\nSRR001      Illumina    ChIP        WT          1         CTCF       HEK293     dm6\nSRR002      Illumina    ChIP        WT          2         CTCF       HEK293     dm6\nSRR003      Illumina    Input       WT          1         None       HEK293     dm6\nSRR004      Illumina    ChIP        KO          1         H3K27me3   HEK293     dm6\nSRR005      Illumina    ChIP        KO          2         H3K27me3   HEK293     dm6\nSRR006      Illumina    Input       KO          1         None       HEK293     dm6\n\n\n\n\nMake sure your file is valid before starting the pipeline:\n\nValidate schema:\n\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\nValidate metadata:\n\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\nâœ”ï¸ You should see â€œValidation Passedâ€ if everything is correct.",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/mapping.html#creating-the-mapping.tsv-file-metadata-table",
    "href": "content/mapping.html#creating-the-mapping.tsv-file-metadata-table",
    "title": "Preprocessing Modules",
    "section": "",
    "text": "The metadata file is crucial for automating sample grouping, identifying replicates, and selecting the appropriate peak caller.\n\nğŸ§  Filename required: metadata/mapping.tsv\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nSample_ID\nUnique ID matching FASTQ or BAM files\nSRR123456\n\n\nInstrument\nSequencing instrument or platform\nIllumina\n\n\nSample_Type\nChIP, Input, IgG, Mock, etc.\nChIP\n\n\nCondition\nExperimental condition or group\nTreated, WT\n\n\nReplicate\nReplicate number (1, 2, 3, â€¦)\n1\n\n\nTarget\nTranscription factor or histone mark\nH3K27ac, CTCF\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nCell_line\nCell line used\nHEK293\n\n\nSpike_Type\nIf spike-in is used (e.g., dm6, ERCC)\ndm6\n\n\n\n\n\n\nSample_ID   Instrument  Sample_Type Condition   Replicate   Target  Cell_line   Spike_Type\nSRR001      Illumina    ChIP        WT          1         CTCF       HEK293     dm6\nSRR002      Illumina    ChIP        WT          2         CTCF       HEK293     dm6\nSRR003      Illumina    Input       WT          1         None       HEK293     dm6\nSRR004      Illumina    ChIP        KO          1         H3K27me3   HEK293     dm6\nSRR005      Illumina    ChIP        KO          2         H3K27me3   HEK293     dm6\nSRR006      Illumina    Input       KO          1         None       HEK293     dm6\n\n\n\n\nMake sure your file is valid before starting the pipeline:\n\nValidate schema:\n\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\nValidate metadata:\n\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\nâœ”ï¸ You should see â€œValidation Passedâ€ if everything is correct.",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/mapping.html#what-is-mapping.tsv-and-why-is-it-important",
    "href": "content/mapping.html#what-is-mapping.tsv-and-why-is-it-important",
    "title": "Preprocessing Modules",
    "section": "2 ğŸ“‹ What is mapping.tsv and Why Is It Important?",
    "text": "2 ğŸ“‹ What is mapping.tsv and Why Is It Important?\n\n2.1 ğŸ” What is it?\nThe mapping.tsv is a metadata tableâ€”a plain text file in tab-separated formatâ€”that contains structured information about each sample in your ChIP-seq experiment. Each row represents a sequencing sample, and each column provides key attributes (e.g., sample ID, condition, replicate number, target protein, etc.).\nThis file needs to be stored at:\nmetadata/mapping.tsv\nThis mapping table will be transformed during the pipeline process: at then you will have a backup (mapping.tsv.bak), mapping_filtered.tsv (produce after replicates QC), and mapping_scaled.tsv (produced after bam cleaning, it contains the ratio of exogenous spike , if spike was detected, and the spike genome type).\n\n\n2.2 ğŸ§  Why is it needed?\nThis file is critical for enabling your pipeline to:\nâœ… Recognize and organize samples automatically âœ… Group replicates (for IDR and reproducibility checks) âœ… Assign controls (Input, IgG, Mock) correctly for peak calling âœ… Select peak caller styles (e.g., narrowPeak for TFs, broadPeak for histone marks) âœ… Validate input consistency using the YAML schema\nWithout this file, the pipeline wouldnâ€™t know how your samples relate to each otherâ€”or how to process them correctly.\n\n\n2.3 ğŸ”— How does the pipeline use it?\n\nPipeline 1: The script 11_Renaming_bam.sh uses the metadata to automatically rename BAM files and organize them into logical groups.\nPipeline 2: Several scripts (e.g., 01_replicate_qc.sh, 03_1_MACS3_peak_calling.sh) read mapping.tsv to apply the right rules for:\n\nSample pairing (ChIP vs.Â Input)\nReplicate merging\nPeak calling type\nReproducibility evaluation (IDR)\n\nValidation scripts (validate_mapping.py, validate_mapping_yaml.sh) ensure the file is complete and follows strict format rules before any analysis starts.\n\n\n\n2.4 ğŸ§¬ Summary\n\n\n\n\n\n\n\nFeature\nWhy it matters\n\n\n\n\nAutomation\nRemoves manual handling of sample groups\n\n\nReproducibility\nEnsures the same logic applies every time\n\n\nCompatibility\nLets the pipeline work across diverse experimental designs\n\n\nQuality control\nPrevents broken analyses due to misannotated samples",
    "crumbs": [
      "Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/overview.html",
    "href": "content/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This pipeline was developed and tested on Linux with POSIX-compliant behavior, macOS compatibility is not supported, If adjustments are needed for your system, you are free to modify the setup accordingly. This is the first version and still in beta test. However you can download and tested in your system as full package or use every modular script. The goal was to create a first Github project to establish Bioinformatic competence by the author.\nThis project implements a modular and reproducible ChIP-seq analysis pipeline designed for both human and mouse samples. The pipeline is organized into two main parts:\n\nPreprocessing and BAM Cleaning â€“ Modular pipeline for data preparation.\nPeak Calling and Reproducibility Analysis â€“ Peak detection, quality control, and IDR-based reproducibility assessment.\n\nThe pipeline supports both HOMER and MACS3 workflows, and uses metadata-based automation. Its structure mirrors the ENCODE ChIP-seq processing pipeline, providing a modular, standardized workflow to produce reproducible peak calls, generate signal tracks, and perform quality assessments such as IDR (Irreproducible Discovery Rate), while automating sample naming and grouping.\nThis package is intended to be flexible and modular. Each module can be customized by the user, or the pipeline can be run as-is. It is not a beginner-friendly pipeline, nor was it designed to be. The pipeline runs directly in Bash on a Linux system. Docker files are included to allow execution within containers, and a brief guide is provided at the end of this text, explaining how to build the container and mount the required folders properly.\nTo run this pipeline you will need to have your paired fastq.gz samples in the samples folder, a mapping.tsv data, and to set the reference files in the Reference folder and the spike reference genomes in the SpikeinReference folder.\nThe package provide two dataset test from GEO dataset to test Hg38 and mm10 samples.\nInformation about these two GEO datasets is provided in the README.md file inside the examples/ folder. The pipeline1 is adapted to automatically download an SRR_Acc_List.txt if the list in the metadata folder. We also provide the mapping.tsv as example",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#overview",
    "href": "content/overview.html#overview",
    "title": "Overview",
    "section": "",
    "text": "This pipeline was developed and tested on Linux with POSIX-compliant behavior, macOS compatibility is not supported, If adjustments are needed for your system, you are free to modify the setup accordingly. This is the first version and still in beta test. However you can download and tested in your system as full package or use every modular script. The goal was to create a first Github project to establish Bioinformatic competence by the author.\nThis project implements a modular and reproducible ChIP-seq analysis pipeline designed for both human and mouse samples. The pipeline is organized into two main parts:\n\nPreprocessing and BAM Cleaning â€“ Modular pipeline for data preparation.\nPeak Calling and Reproducibility Analysis â€“ Peak detection, quality control, and IDR-based reproducibility assessment.\n\nThe pipeline supports both HOMER and MACS3 workflows, and uses metadata-based automation. Its structure mirrors the ENCODE ChIP-seq processing pipeline, providing a modular, standardized workflow to produce reproducible peak calls, generate signal tracks, and perform quality assessments such as IDR (Irreproducible Discovery Rate), while automating sample naming and grouping.\nThis package is intended to be flexible and modular. Each module can be customized by the user, or the pipeline can be run as-is. It is not a beginner-friendly pipeline, nor was it designed to be. The pipeline runs directly in Bash on a Linux system. Docker files are included to allow execution within containers, and a brief guide is provided at the end of this text, explaining how to build the container and mount the required folders properly.\nTo run this pipeline you will need to have your paired fastq.gz samples in the samples folder, a mapping.tsv data, and to set the reference files in the Reference folder and the spike reference genomes in the SpikeinReference folder.\nThe package provide two dataset test from GEO dataset to test Hg38 and mm10 samples.\nInformation about these two GEO datasets is provided in the README.md file inside the examples/ folder. The pipeline1 is adapted to automatically download an SRR_Acc_List.txt if the list in the metadata folder. We also provide the mapping.tsv as example",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#pipeline-components",
    "href": "content/overview.html#pipeline-components",
    "title": "Overview",
    "section": "2 ğŸ“š Pipeline Components",
    "text": "2 ğŸ“š Pipeline Components\n\n2.1 ğŸ”§ 1. Read Preprocessing and Alignment\nInput: FASTQ files (reads)\nTools Used:\n\ncutadapt: Adapter trimming\nBWA: Alignment to reference genome\nSamtools, Picard: Sorting, deduplication, indexing, BAM cleanup\n\nOutputs:\n\nUnfiltered alignments\nFiltered, deduplicated BAM files\nRemoval of excluded/blacklisted regions (e.g., ENCODE blacklist)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/overview.html#replicate-quality-control-with-deeptools",
    "href": "content/overview.html#replicate-quality-control-with-deeptools",
    "title": "Overview",
    "section": "3 ğŸ”¬ Replicate Quality Control with deepTools",
    "text": "3 ğŸ”¬ Replicate Quality Control with deepTools\nThis pipeline uses deepTools (e.g., plotFingerprint, multiBamSummary) to evaluate replicate quality.\n\n3.1 âœ… Advantages of deepTools:\n\nğŸ§ª Modern and actively maintained â€“ compatible with recent Python environments\nğŸ“Š Fingerprint and cross-correlation plots â€“ assess ChIP enrichment and background\nâš™ï¸ Format flexibility â€“ supports BAM and BigWig\nğŸ³ Container-friendly â€“ easy to integrate in Docker-based workflows\nğŸ“ Multi-sample analysis â€“ supports multiBamSummary, plotCorrelation\n\nUsers can optionally replace this with Relative Strand Correlation (RSC) via the ChIPQC R package by editing the relevant module â€” showcasing the pipelineâ€™s modularity.\n\n\n\n3.2 â“ Why not use phantompeakqualtools?\nAlthough once widely used to assess replicate quality, phantompeakqualtools is now:\n\nğŸ›‘ Outdated and no longer actively maintained\nğŸª Depends on legacy Perl environments and outdated tools like CClang v5\nğŸ§± Hard to containerize or integrate into modern pipelines\n\nTherefore, it has been replaced by more maintainable alternatives in this workflow.\n\n\n\n3.3 ğŸ“ˆ 2. Signal Generation\nTools:\n\nMACS3: Generates signal tracks (bedGraph or bigWig)\nBEDTools: For further manipulation\n\nOutputs:\n\nSignal p-value tracks\nFold change over control tracks\n\n\n\n\n3.4 ğŸ“ 3. Peak Calling\nPerformed on:\n\nIndividual replicates\nPooled replicates\nPseudoreplicates\n\nTools:\n\nMACS3 or HOMER\n\nOutputs:\n\nReplicated peaks\nPooled peaks\nPseudoreplicated peaks\n\n\n\n\n3.5 ğŸ“Š 4. IDR Analysis\nTool: idr â€” used to assess peak reproducibility\nThis pipeline uses a patched IDR version compatible with the latest numpy, avoiding the need to downgrade.\n\n\n\n3.6 ğŸ›  Patch + Install IDR (NumPy Compatibility)\nbash\n    wget https://github.com/kundajelab/idr/archive/refs/tags/2.0.4.2.tar.gz && \\\n    tar -xvf 2.0.4.2.tar.gz && \\\n    cd idr-2.0.4.2/idr && \\\n    sed -i 's/numpy.int/int/g' idr.py && \\\n    cd .. && \\\n    pip install . --break-system-packages && \\\n    rm -rf /opt/idr-2.0.4.2 /opt/2.0.4.2.tar.gz\nOutputs:\n\nIDR-ranked peaks\nIDR-thresholded peaks\nConservative IDR peaks\n\n\n\n\n3.7 ğŸ” 5. Replicate/Partition Concordance\nAssesses consistency of peaks across replicates and pseudoreplicates.\n\n\n\n3.8 ğŸ§¾ 6. Format Conversion and Final Outputs\nCovers conversion to final formats for visualization, downstream analysis, or submission.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/installation.html",
    "href": "content/installation.html",
    "title": "âš™ï¸ Installation Guide",
    "section": "",
    "text": "This guide explains how to set up the ChIP-seq Modular Analysis Pipeline using:\n\nâœ… Git clone (recommended)\nğŸ“ .tar.gz or .zip archive download\nğŸ³ Docker (see Docker Guide)\nğŸ§ª Conda environments (see Conda Setup)",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#installation-overview",
    "href": "content/installation.html#installation-overview",
    "title": "âš™ï¸ Installation Guide",
    "section": "",
    "text": "This guide explains how to set up the ChIP-seq Modular Analysis Pipeline using:\n\nâœ… Git clone (recommended)\nğŸ“ .tar.gz or .zip archive download\nğŸ³ Docker (see Docker Guide)\nğŸ§ª Conda environments (see Conda Setup)",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#option-1-clone-from-github",
    "href": "content/installation.html#option-1-clone-from-github",
    "title": "âš™ï¸ Installation Guide",
    "section": "2 ğŸ“‚ Option 1: Clone from GitHub",
    "text": "2 ğŸ“‚ Option 1: Clone from GitHub\nThe most robust and update-safe method:\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\n\n2.1 âœ… Make Scripts Executable (already github executable commit, but in case)\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#option-2-download-archive",
    "href": "content/installation.html#option-2-download-archive",
    "title": "âš™ï¸ Installation Guide",
    "section": "3 ğŸ“¦ Option 2: Download Archive",
    "text": "3 ğŸ“¦ Option 2: Download Archive\n\n3.1 ğŸ”¸ If you downloaded the .tar.gz version:\ntar -xzvf your-pipeline.tar.gz\ncd your-pipeline\nScripts remain executable â€” no chmod needed.\n\n\n3.2 âš ï¸ If you downloaded the .zip version:\nunzip your-pipeline.zip\ncd your-pipeline\nThen run:\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#test-the-installation",
    "href": "content/installation.html#test-the-installation",
    "title": "âš™ï¸ Installation Guide",
    "section": "4 âœ… Test the Installation",
    "text": "4 âœ… Test the Installation\nMake sure the scripts work:\n./run_pipeline1.sh --help\n./run_pipeline2.sh --help",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#expected-file-structure",
    "href": "content/installation.html#expected-file-structure",
    "title": "âš™ï¸ Installation Guide",
    "section": "5 ğŸ“ Expected File Structure",
    "text": "5 ğŸ“ Expected File Structure\nyour-repo/\nâ”œâ”€â”€ run_pipeline1.sh\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/\nâ”‚   â””â”€â”€ pipeline1/\nâ”‚   â””â”€â”€ pipeline2/\nâ”‚   â””â”€â”€ utils/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ analysis/\nâ””â”€â”€ ...",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#related-setup-pages",
    "href": "content/installation.html#related-setup-pages",
    "title": "âš™ï¸ Installation Guide",
    "section": "6 ğŸ“š Related Setup Pages",
    "text": "6 ğŸ“š Related Setup Pages\n\nğŸ“¦ Reference Setup\nğŸ§¬ Metadata & Mapping Format\nğŸ”§ Software Requirements\nğŸ³ Docker Setup\nğŸ§ª Conda Environment",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "content/installation.html#notes",
    "href": "content/installation.html#notes",
    "title": "âš™ï¸ Installation Guide",
    "section": "7 ğŸ“ Notes",
    "text": "7 ğŸ“ Notes\nIf you encounter permission errors or missing dependencies, refer to Troubleshooting.\n\n---\n\nLet me know if you want to include a section for installing a local Conda environment (`environment.yml`) or a helper `install.sh`.",
    "crumbs": [
      "âš™ï¸ Installation Guide"
    ]
  },
  {
    "objectID": "docker1_table.html",
    "href": "docker1_table.html",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "0.1 ğŸ“ Explanation\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--rm\nRemoves container after run\n\n\n-v &lt;host&gt;:&lt;container&gt;\nMounts folders into container\n\n\n-t 4\nNumber of threads\n\n\n-r mm10\nReference genome (hg38, mm10, etc.)\n\n\n-a tn5_truseq\nAdapter type (should match whatâ€™s in your pipeline)\n\n\npipeline1:latest\nDocker image name"
  },
  {
    "objectID": "content/docker_pipeline1.html",
    "href": "content/docker_pipeline1.html",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "",
    "text": "This guide explains how to build and run the pipeline1 container for the ChIP-seq Modular Analysis Pipeline using Docker. It is beginner-friendly and reproducible.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#what-is-this",
    "href": "content/docker_pipeline1.html#what-is-this",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "",
    "text": "This guide explains how to build and run the pipeline1 container for the ChIP-seq Modular Analysis Pipeline using Docker. It is beginner-friendly and reproducible.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#prerequisites",
    "href": "content/docker_pipeline1.html#prerequisites",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "2 âœ… Prerequisites",
    "text": "2 âœ… Prerequisites\nMake sure you have:\n\nDocker installed\nA terminal open in the project root (chip-seq-modular-pipeline/)",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#step-1-build-the-docker-image",
    "href": "content/docker_pipeline1.html#step-1-build-the-docker-image",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "3 ğŸ—ï¸ Step 1: Build the Docker Image",
    "text": "3 ğŸ—ï¸ Step 1: Build the Docker Image\nFrom the project root, run:\ndocker build -f Docker/Dockerfile.pipeline1 -t pipeline1-test .\nWhere:\n\n-f tells Docker which file to use\n-t pipeline1-test names the image\n. sets the build context (your entire repo)\n\n\nâ±ï¸ First-time builds may take several minutes.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#step-2-folder-structure",
    "href": "content/docker_pipeline1.html#step-2-folder-structure",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "4 ğŸ“ Step 2: Folder Structure",
    "text": "4 ğŸ“ Step 2: Folder Structure\nTo ensure correct volume mounting inside the container, organize your folders like this:\n\n\n\n\n\n\n\n\nHost Folder\nContainer Mount\nDescription\n\n\n\n\nsamples/\n/pipeline/samples\nRaw sequencing input\n\n\nReference/\n/pipeline/Reference\nGenome reference files\n\n\nSpikeinReference/\n/pipeline/SpikeinReference\nSpike-in configs\n\n\nmetadata/\n/pipeline/metadata\nSample metadata and mapping files\n\n\ntemplates/\n/pipeline/templates\nYAML/TSV configuration templates\n\n\nadapters/\n/pipeline/adapters\nAdapter files for trimming\n\n\nassets/\n/pipeline/assets\nIcons, logos, images, PDFs, etc.\n\n\nmodules/\n/pipeline/modules\nPipeline logic and scripts\n\n\nresults/\n/pipeline/results\nOutput from pipeline1\n\n\nanalysis/\n/pipeline/analysis\nOutput passed to pipeline2\n\n\n\n\nğŸ› ï¸ Docker will auto-create results/ and analysis/ if they donâ€™t exist.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#step-3-run-the-pipeline-default",
    "href": "content/docker_pipeline1.html#step-3-run-the-pipeline-default",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "5 â–¶ï¸ Step 3: Run the Pipeline (Default)",
    "text": "5 â–¶ï¸ Step 3: Run the Pipeline (Default)\nTo run the default pipeline with standard parameters:\ndocker run --rm -it \\\n  -v \"$PWD/samples:/pipeline/samples\" \\\n  -v \"$PWD/Reference:/pipeline/Reference\" \\\n  -v \"$PWD/SpikeinReference:/pipeline/SpikeinReference\" \\\n  -v \"$PWD/metadata:/pipeline/metadata\" \\\n  -v \"$PWD/templates:/pipeline/templates\" \\\n  -v \"$PWD/adapters:/pipeline/adapters\" \\\n  -v \"$PWD/assets:/pipeline/assets\" \\\n  -v \"$PWD/modules:/pipeline/modules\" \\\n  -v \"$PWD/results:/pipeline/results\" \\\n  -v \"$PWD/analysis:/pipeline/analysis\" \\\n  pipeline1-test bash run_pipeline1.sh",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#step-4-run-with-custom-options",
    "href": "content/docker_pipeline1.html#step-4-run-with-custom-options",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "6 âš™ï¸ Step 4: Run with Custom Options",
    "text": "6 âš™ï¸ Step 4: Run with Custom Options\nExample with flags for genome, adapter, platform, and threads:\ndocker run --rm -it \\\n  -v \"$PWD/samples:/pipeline/samples\" \\\n  -v \"$PWD/Reference:/pipeline/Reference\" \\\n  -v \"$PWD/SpikeinReference:/pipeline/SpikeinReference\" \\\n  -v \"$PWD/metadata:/pipeline/metadata\" \\\n  -v \"$PWD/templates:/pipeline/templates\" \\\n  -v \"$PWD/adapters:/pipeline/adapters\" \\\n  -v \"$PWD/assets:/pipeline/assets\" \\\n  -v \"$PWD/modules:/pipeline/modules\" \\\n  -v \"$PWD/results:/pipeline/results\" \\\n  -v \"$PWD/analysis:/pipeline/analysis\" \\\n  pipeline1-test bash run_pipeline1.sh \\\n    -t 8 \\\n    -r mm10 \\\n    -a tn5_nextera \\\n    -p IONTORRENT \\\n    -m metadata/mapping.tsv \\\n    -f csv",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#troubleshooting-tips",
    "href": "content/docker_pipeline1.html#troubleshooting-tips",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "7 ğŸª› Troubleshooting & Tips",
    "text": "7 ğŸª› Troubleshooting & Tips\n\nâœ… Run Docker inside your project folder.\nâœ… Mount results/ and analysis/ to keep output.\nğŸ To debug interactively:\n\ndocker run --rm -it pipeline1-test\nThen inside the container:\nbash run_pipeline1.sh -t 4 -r hg38",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#next-steps",
    "href": "content/docker_pipeline1.html#next-steps",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "8 ğŸ¯ Next Steps",
    "text": "8 ğŸ¯ Next Steps\nOnce pipeline1 finishes, you can move to pipeline2 using analysis/ as input.",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/docker_pipeline1.html#output-overview",
    "href": "content/docker_pipeline1.html#output-overview",
    "title": "ğŸ³ Running Pipeline1 with Docker",
    "section": "9 ğŸ“‚ Output Overview",
    "text": "9 ğŸ“‚ Output Overview\n\n\n\nFolder\nDescription\n\n\n\n\nresults/\nOutput results from pipeline1\n\n\nanalysis/\nUsed later as input for pipeline2\n\n\nlogs/\n(optional) runtime logs if implemented",
    "crumbs": [
      "ğŸ³ Running Pipeline1 with Docker"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "Welcome to the ChIP-seq Modular Analysis Pipeline documentation!\nThis site provides a detailed manual for running and understanding the modular ChIP-seq pipeline developed by Nancy Anderson. It includes setup, usage, modules, metadata structure, Docker/Conda environments, and more.\n\n\n\n\nOverview\nPipeline 1: QC & Preprocessing\nPipeline 2: Peak Calling & Analysis\nMetadata Guide\nDocker Usage - Pipeline 1\nDocker Usage - Pipeline 2\nUnified Docker Guide\nConda Environment Setup\nSoftware Requirements\nTroubleshooting\nCitations\nLicense\n\n\n\n\n\nIf youâ€™re eager to run the pipeline quickly, jump to:\nğŸ‘‰ Quick Start Guide\nOr explore the Docker options for portable setup:\nğŸ‘‰ Docker Setup",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/index.html#sections",
    "href": "content/index.html#sections",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "Overview\nPipeline 1: QC & Preprocessing\nPipeline 2: Peak Calling & Analysis\nMetadata Guide\nDocker Usage - Pipeline 1\nDocker Usage - Pipeline 2\nUnified Docker Guide\nConda Environment Setup\nSoftware Requirements\nTroubleshooting\nCitations\nLicense",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/index.html#quick-start",
    "href": "content/index.html#quick-start",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "",
    "text": "If youâ€™re eager to run the pipeline quickly, jump to:\nğŸ‘‰ Quick Start Guide\nOr explore the Docker options for portable setup:\nğŸ‘‰ Docker Setup",
    "crumbs": [
      "ChIP-seq Modular Analysis Pipeline"
    ]
  },
  {
    "objectID": "content/license.html",
    "href": "content/license.html",
    "title": "ğŸ“„ MIT License",
    "section": "",
    "text": "1 MIT License\nCopyright (c) 2025 Nancy Anderson\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the â€œSoftwareâ€), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "crumbs": [
      "ğŸ“„ MIT License"
    ]
  },
  {
    "objectID": "content/pipeline1.html#supported-data-types",
    "href": "content/pipeline1.html#supported-data-types",
    "title": "ğŸ”„ Pipeline 1: QC & Preprocessing Modules",
    "section": "2 ğŸ§¬ Supported Data Types",
    "text": "2 ğŸ§¬ Supported Data Types\nThis pipeline is designed for ChIP-seq and ATAC-seq data preprocessing. It includes:\n\nAdapter trimming (cutadapt)\nParallel quality control (FastQC)\nOptional spike-in filtering\nMitochondrial read removal\nBlacklist region filtering (ENCODE)\nClean alignment with BWA and BAM post-processing\n\nBy supporting these steps, the pipeline ensures high-quality, reproducible inputs for downstream peak calling or chromatin accessibility analysis.\n\nğŸ” While optimized for ChIP-seq, the pipeline is fully compatible with ATAC-seq experiments.\n\nScripts: 01_init_directories.sh to 11_Renaming_bam.sh\nmodules/pipeline1\\$ tree . \nâ”œâ”€â”€ 01_init_directories.sh \nâ”œâ”€â”€ 02_reference_check.sh \nâ”œâ”€â”€ 03_input_fetch.sh \nâ”œâ”€â”€ 04_fastqc_parallel.sh \nâ”œâ”€â”€ 05_05_Cutadapt_trimming_phix_parallel.sh \nâ”œâ”€â”€ 06_fastqc_trimmed_parallel.sh \nâ”œâ”€â”€ 07_spike_detect.sh \nâ”œâ”€â”€ 08_alignment_bwa_spike.sh \nâ”œâ”€â”€ 09_readgroups_add.sh \nâ”œâ”€â”€ 10_bam_cleaning.sh \nâ”œâ”€â”€ 10_plot_spike_qc_summary.R \nâ””â”€â”€ 11_Renaming_bam.sh\nGreat structure! Given the flow of your Quarto document, the best place to add the usage documentation for run_pipeline1.sh would be after the ## ğŸ§¬ Supported Data Types section and before the ### ğŸ”§ Modules section.\nThis keeps your document logically organized:\n\nIntro\nWhat data it supports\nHow to run it (Usage) âœ…\nWhat each module does\nWhat the outputs are\n\n\n\n2.1 âœ… Suggested Update (add this block in your .qmd file):\n### ğŸš€ Usage: `run_pipeline1.sh`\n\nThis script is the entry point for Pipeline 1 and supports both default and customized executions.\n\n#### âœ… Default Run\n\nRun with all defaults:\n```bash\nbash run_pipeline1.sh\nThis uses:\n\nTHREADS = 4\nREFERENCE = hg38\nADAPTER = tn5_truseq\nPLATFORM = ILLUMINA\nMAPPING = metadata/mapping.tsv\nREPORT_FORMAT = human\n\n\n2.1.1 ğŸ”§ Full Custom Run\nExample of overriding all defaults:\nbash run_pipeline1.sh -t 8 -r mm10 -a tn5_nextera -p IONTORRENT -m data/sample_metadata.tsv -f csv\nOverrides:\n\n-t 8: 8 threads\n-r mm10: mouse genome\n-a tn5_nextera: alternative adapter\n-p IONTORRENT: sequencing platform\n-m data/sample_metadata.tsv: custom mapping file\n-f csv: report format\n\n\n\n2.1.2 â„¹ï¸ Help\nFor option details:\nbash run_pipeline1.sh --help\n\n\n\n2.2 ğŸ”§ Modules\n\nğŸ”§ Modules Overview\n\n\nScript\nPurpose\n\n\n\n\n01_init_directories.sh\nCreates necessary directory structure.\n\n\n02_reference_check.sh\nValidates presence of genome reference files.\n\n\n03_input_fetch.sh\nFetches and decompresses user-supplied FASTQ files.\n\n\n04_fastqc_parallel.sh\nRuns FastQC and generates quality reports.\n\n\n05_Cutadapt_trimming_phix_parallel.sh\nTrims adapters and low-quality bases using Cutadapt.\n\n\n06_fastqc_trimmed_parallel.sh\nRuns FastQC on trimmed reads.\n\n\n07_spike_detect.sh\nDetects exogenous spike-in if not provided.\n\n\n08_alignment_bwa_spike.sh\nAligns reads using BWA and checks spike content.\n\n\n09_readgroups_add.sh\nAdds read group metadata using Picard.\n\n\n10_bam_cleaning.sh\nCleans and indexes BAMs; performs spike-in QC.\n\n\n11_Renaming_bam.sh\nRenames BAMs using mapping.tsv for grouping.\n\n\n\n\n\n2.3 ğŸ“ Output Folders\nresults/: Raw outputs and intermediate files.\nanalysis/Renamed_Cleaned: Cleaned and filtered BAMs for downstream usage.\nresults/\nresults/Filtered$ tree \n\nâ”œâ”€â”€ BAM}\nâ”œâ”€â”€ Filtered\nâ”œ        â”œâ”€â”€ Cleaned  âœ… here are the cleaned BAM that will be renamed. â”‚\nâ”œ        â”œâ”€â”€ Deduplicated\nâ”œ        â””â”€â”€ Metrics\nâ”œâ”€â”€ QC_fastqc\nâ”œâ”€â”€ QC_spike_plots\nâ”œâ”€â”€ QC_trimmed_fastqc\nâ”œâ”€â”€ spike_analysis\nâ””â”€â”€ Trimmed",
    "crumbs": [
      "ğŸ”„ Pipeline 1: QC & Preprocessing Modules"
    ]
  },
  {
    "objectID": "content/reference_setup.html",
    "href": "content/reference_setup.html",
    "title": "ğŸ§¬ Initial documents and reference setups",
    "section": "",
    "text": "The following helper scripts are available in the modules/utils/ directory for setup and validation:\n\n\nAutomates the preparation of genome reference directories and indexes.\nUsage:\nbash modules/utils/setup_reference.sh &lt;GENOME_NAME&gt; &lt;GENOME_FASTA&gt;\nExample:\nbash modules/utils/setup_reference.sh hg38 /path/to/hg38.fa\nWhat it does:\n\nCreates a genome folder under resources/genomes/GENOME_NAME/\nGenerates:\n\n.fai index via samtools faidx\nbwa index (bwa index)\nChromosome sizes (.chrom.sizes)\nDictionary via picard CreateSequenceDictionary\n\n\n\n\n\nPrepares spike-in reference genomes similarly to the main genome.\nUsage:\nbash modules/utils/setup_spikein_refs.sh &lt;SPIKE_NAME&gt; &lt;SPIKE_FASTA&gt;\nExample:\nbash modules/utils/setup_spikein_refs.sh dm6 /path/to/dm6.fa\nCreates:\n\nIndexed bwa genome and dictionary for the spike\nOrganized under resources/spikein/SPIKE_NAME/\n\nThese scripts must be run before alignment steps in the pipeline to ensure all genome/spike references are available and indexed properly.\nLet me know if youâ€™d like a Makefile target to wrap these commands for common genomes (e.g., hg38 + dm6).",
    "crumbs": [
      "ğŸ§¬ Initial documents and reference setups"
    ]
  },
  {
    "objectID": "content/reference_setup.html#utilities",
    "href": "content/reference_setup.html#utilities",
    "title": "ğŸ§¬ Initial documents and reference setups",
    "section": "",
    "text": "The following helper scripts are available in the modules/utils/ directory for setup and validation:\n\n\nAutomates the preparation of genome reference directories and indexes.\nUsage:\nbash modules/utils/setup_reference.sh &lt;GENOME_NAME&gt; &lt;GENOME_FASTA&gt;\nExample:\nbash modules/utils/setup_reference.sh hg38 /path/to/hg38.fa\nWhat it does:\n\nCreates a genome folder under resources/genomes/GENOME_NAME/\nGenerates:\n\n.fai index via samtools faidx\nbwa index (bwa index)\nChromosome sizes (.chrom.sizes)\nDictionary via picard CreateSequenceDictionary\n\n\n\n\n\nPrepares spike-in reference genomes similarly to the main genome.\nUsage:\nbash modules/utils/setup_spikein_refs.sh &lt;SPIKE_NAME&gt; &lt;SPIKE_FASTA&gt;\nExample:\nbash modules/utils/setup_spikein_refs.sh dm6 /path/to/dm6.fa\nCreates:\n\nIndexed bwa genome and dictionary for the spike\nOrganized under resources/spikein/SPIKE_NAME/\n\nThese scripts must be run before alignment steps in the pipeline to ensure all genome/spike references are available and indexed properly.\nLet me know if youâ€™d like a Makefile target to wrap these commands for common genomes (e.g., hg38 + dm6).",
    "crumbs": [
      "ğŸ§¬ Initial documents and reference setups"
    ]
  },
  {
    "objectID": "content/pipeline2.html",
    "href": "content/pipeline2.html",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "",
    "text": "Pipeline2 continues the analysis after preprocessing and alignment, performing replicate QC, peak calling, and reproducibility evaluation via IDR. It supports both MACS3 and HOMER peak callers, with outputs from both individual and pooled replicates.\nDuring the IDR evaluation, a series of QC analyses is performed, and an HTML/PDF report is generated and stored at:\nThe report includes and are in HTML and pdf format:",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#functional-enrichment-report",
    "href": "content/pipeline2.html#functional-enrichment-report",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "1 ğŸ§¬ Functional Enrichment Report",
    "text": "1 ğŸ§¬ Functional Enrichment Report\nThe final enrichment summary report (including Gene Ontology, KEGG, Reactome, and g:Profiler figures) is available as a downloadable PDF and HTML. If the PRE_POSSIBLE_OUTCOME.tsv file is too large, the PDF will include only the g:Profiler figures. The full PRE_POSSIBLE_OUTCOME.tsv content will remain available in its original TSV format and within the HTML report.\n\n1.1 ğŸ“ Example Reports\nğŸ‘‰ hg38: ğŸ“ View report (PDF)\nğŸ‘‰ mm10: ğŸ“ View report (PDF)\n\n\n\n1.2 ğŸš€ Usage: run_pipeline2.sh\nThis script runs Pipeline 2 â€” performing peak calling, replicate QC, IDR analysis, and functional annotation using MACS3 and HOMER.\n\n1.2.1 âœ… Default Run\nRun with all default parameters:\nbash run_pipeline2.sh\nThis will use:\n\nREFERENCE = hg38\nMAPPING = metadata/mapping.tsv\nPEAK_CALLER = both (MACS3 and HOMER)\nTHREADS = 4\nIDR_THRESHOLD = 0.05\nTARGET_RULES = templates/target_peak_rules.yaml\nTSS_DB = HOMER (for annotation)\n\n\nâœ… Defaults are suitable for most ChIP-seq and ATAC-seq use cases.\n\n\n\n\n1.2.2 ğŸ”§ Full Custom Run\nCustomize all parameters:\nbash run_pipeline2.sh \\\n  -r mm10 \\\n  -m data/sample_metadata.tsv \\\n  -p macs3 \\\n  -t 8 \\\n  -i 0.01 \\\n  -y templates/target_peak_rules_HOMER.yaml \\\n  --tss-db clusterProfiler\nExplanation:\n\n-r mm10 â†’ Use mouse genome\n-m data/sample_metadata.tsv â†’ Custom metadata file\n-p macs3 â†’ Use MACS3 only (can be macs3, homer, or both)\n-t 8 â†’ Use 8 threads\n-i 0.01 â†’ Adjust IDR threshold\n-y templates/target_peak_rules_HOMER.yaml â†’ Custom target rules\n--tss-db clusterProfiler â†’ Use clusterProfiler instead of HOMER for gene annotation.\n\n\n\n\n\n\n\n#### â„¹ï¸ Help\n\n\n\n\n### ğŸ“„ 1. mapping_schema.yaml â€” Metadata Validation Rules\n\n\nDefines structure and validation logic for your mapping.tsv metadata.\n\n\n#### ğŸ” Key Elements:\n\n\n- Required fields: Sample_ID, Instrument, Sample_Type, Condition, Replicate, Target - Optional fields: Cell_line, Spike_Type - Regex validation: For fields like Sample_Type, Replicate\n\n\n#### âœ… Validate Schema File\n\n\nbash bash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml #### âœ… Validate Metadata File\n\n\nbash python3 modules/utils/validate_mapping.py \\ --mapping metadata/mapping.tsv \\ --schema templates/mapping_schema.yaml\n\n\n\n\n\n\n1.3 ğŸ“„ 2. target_peak_rules.yaml â€” Peak Calling Styles (MACS3)\nThis config determines which targets require narrow or broad peak calling.\nExamples:\n\nCTCF, MYC â†’ narrowPeak\nH3K27me3, EZH2 â†’ broadPeak\n\n\n\n\n1.4 ğŸ“„ 3. target_peak_rules_HOMER.yaml â€” Peak Calling Styles (HOMER)\nClassifies targets by style for findPeaks:\n\nfactor â†’ e.g., MYC, SOX2\nhistone â†’ e.g., H3K27me3, EZH2\nGROseq, DNaseI, etc.\n\n\nâš ï¸ Note: These files are pre-validated. If you add new targets, you must write your own validation.",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "href": "content/pipeline2.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh",
    "text": "2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh\nmodules/pipeline2/\nâ”œâ”€â”€ 01_replicate_qc.sh\nâ”œâ”€â”€ 02_merge_pseudoreplicates.sh\nâ”œâ”€â”€ 03_1_MACS3_peak_calling.sh\nâ”œâ”€â”€ 03_2_homer_peak_calling_fold_fdr_relaxed.sh\nâ”œâ”€â”€ 03_2_homer_to_peakFormat.R\nâ”œâ”€â”€ 03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 03_4_homer_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 04_plot_idr_summary.R\nâ”œâ”€â”€ 04_run_idr.sh\nâ”œâ”€â”€ 05_peak_annotation.sh\nâ”œâ”€â”€ 06_igv_snapshot.sh\nâ””â”€â”€ cluster_enrichment_updated_hg_mice.R\n\n2.1 ğŸ”§ Modules\n\n\n\n\n\n\n\nScript\nPurpose\n\n\n\n\n01_replicate_qc.sh\nAnalyzes biological replicates for correlation (cutoff: 0.8), filters low-correlation samples.\n\n\n02_merge_pseudoreplicates.sh\nMerges BAMs and creates pseudo-replicates for IDR.\n\n\n03_1_MACS3_peak_calling.sh\nPeak calling using MACS3 with smart baseline control matching.\n\n\n03_2_homer_peak_calling_fold_fdr_relaxed.sh\nHOMER-based peak calling using target_peak_rules_HOMER.yaml.\n\n\n03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nMACS3 peak calling for pooled/pseudo BAMs.\n\n\n03_4_homer_peak_calling_pooled_pseudoreps.sh\nHOMER peak calling for pooled/pseudo BAMs.\n\n\n04_run_idr.sh\nRuns IDR on replicate pairs, pooled vs individual, etc.\n\n\n05_peak_annotation.sh\nPerforms TSS annotation of peak files.\n\n\n06_igv_snapshot.sh\nGenerates IGV snapshots for peak visualization.\n\n\n\n\n\n\n\n\nâš ï¸ Note :06_igv_snapshot.sh is an extra module not yet adapted to the pipeline. This particular script produce to many snapshots if the user do not define the peaks based on their needs. â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/pipeline2.html#analysis-folder-structure",
    "href": "content/pipeline2.html#analysis-folder-structure",
    "title": "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation",
    "section": "3 ğŸ—‚ Analysis Folder Structure",
    "text": "3 ğŸ—‚ Analysis Folder Structure\nanalysis$ tree -L 1\n.\nâ”œâ”€â”€ BAM_replicate_fail\nâ”œâ”€â”€ ChIPseeker_TSS_Hommer_IDR_annotation\nâ”œâ”€â”€ IDR_Results\nâ”‚   â”œâ”€â”€ homer\nâ”‚   â””â”€â”€ macs3\nâ”œâ”€â”€ PeakCalling_HOMER\nâ”œâ”€â”€ PeakCalling_HOMER_pool_pseudo\nâ”œâ”€â”€ PeakCalling_MACS3\nâ”œâ”€â”€ PeakCalling_MACS3_pool_pseudo\nâ”œâ”€â”€ Pooled_BAMs\nâ”œâ”€â”€ pooling_log.tsv\nâ”œâ”€â”€ Pool_Pseudo_QC_stats\nâ”œâ”€â”€ Pseudoreplicates\nâ”œâ”€â”€ Renamed_Cleaned\nâ””â”€â”€ Replicate_QC\n    â”œâ”€â”€ bigwig\n    â”œâ”€â”€ deeptools\n    â”œâ”€â”€ pbc_metrics.tsv\n    â””â”€â”€ tmp_groups",
    "crumbs": [
      "ğŸ”„ Pipeline 2: Peak Calling, Reproducibility, and Functional Annotation"
    ]
  },
  {
    "objectID": "content/docker_guide.html",
    "href": "content/docker_guide.html",
    "title": "ğŸ³ Docker guide",
    "section": "",
    "text": "Docker provides a reproducible, isolated environment for running the pipeline. Once the image is built, the entire analysis can be executed without requiring local software installations.\n\n0.1 âš™ï¸ Running the Pipeline in Docker\nUse the following command to launch the container:\ndocker run -v \"$PWD:/pipeline2\" -w /pipeline2 pipeline2-image \\\n  bash run_pipeline2.sh [options]\nThis command:\n\nMounts your current directory ($PWD) into the container at /pipeline2\nSets /pipeline2 as the working directory\nExecutes the pipeline script inside the container\n\n\n\n0.2 ğŸ“ Project Structure Inside the Container\nYour pipeline files are accessible under:\n/pipeline2/\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/pipeline2/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ logs/\nâ”œâ”€â”€ analysis/\nRelative paths like modules/pipeline2/04_run_idr.sh will resolve correctly.\n\n\n\n0.3 ğŸ”— Docker Command Summary\n\n\n\n\n\n\n\nStep\nCommand\n\n\n\n\nBuild image\ndocker build -f Dockerfile.pipeline2 -t pipeline2-image .\n\n\nRun pipeline\ndocker run -v \"$PWD:/pipeline2\" -w /pipeline2 pipeline2-image ...\n\n\nView results\nCheck output files in your local analysis/ and logs/ directories\n\n\n\n\nFor advanced options or troubleshooting, refer to the main documentation or contact the maintainers.",
    "crumbs": [
      "ğŸ³ Docker guide"
    ]
  },
  {
    "objectID": "content/Conda.html",
    "href": "content/Conda.html",
    "title": "ğŸ Core enviroment:conda",
    "section": "",
    "text": "Below are two separate Conda environment YAML files:\nThis includes all necessary packages for running the pipeline in a non-Docker setup, excluding R packages and HOMER.\nname: chipseq_pipeline_base\nchannels:\n  - bioconda\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.9\n  - pip\n  - numpy\n  - pandas\n  - pyyaml\n  - samtools&gt;=1.14\n  - bedtools&gt;=2.30.0\n  - bwa\n  - cutadapt&gt;=4.0\n  - macs3\n  - deeptools\n  - jq\n  - r-base=4.2\n  - r-data.table\n  - r-ggplot2\n  - r-dplyr\n  - r-readr\n  - r-tidyr\n  - r-stringr\n  - r-jsonlite\n  - r-httr\n  - r-remotes  # for installing from GitHub\n  - pip:\n      - idr  # we patch it manually later",
    "crumbs": [
      "ğŸ Core enviroment:conda"
    ]
  },
  {
    "objectID": "content/Conda.html#software-requirements-non-docker-setup",
    "href": "content/Conda.html#software-requirements-non-docker-setup",
    "title": "ğŸ Core enviroment:conda",
    "section": "1 âš™ï¸ Software Requirements (Non-Docker Setup)",
    "text": "1 âš™ï¸ Software Requirements (Non-Docker Setup)\nWhile this pipeline is fully Docker-compatible, you can also run it natively by installing the required software through ğŸ Conda and ğŸ“˜ â„ .\n\n1.1 ğŸ“ Environment Files\nAll environment setup files are located in:\nenv/\nâ”œâ”€â”€ env_pipeline_base.yml     # Conda YAML file (base system + CLI tools)\nâ”œâ”€â”€ env_pipeline_r.R          # R script to install required R/Bioconductor packages\n\n\n1.2 ğŸ§ª Step 1: Create Conda Environment\nUse the provided YAML file to create a clean Conda environment with all the command-line tools.\nconda env create -f env/env_pipeline_base.yml\nconda activate chipseq_pipeline_base\n\n\n1.3 ğŸ“¦ Step 2: Install R and Bioconductor Packages\nOnce inside the conda environment, install the required R packages using:\nRscript env/env_pipeline_r.R\nThis script will:\n\nCheck for missing CRAN or Bioconductor dependencies\nInstall them as needed\nLoad the correct annotation packages for both human (hg38) and mouse (mm10) genomes\n\n\n\n1.4 ğŸ§° Tool Dependencies\nSome tools like picard.jar and qualimap are not installed via Conda but are required. These are bundled inside the pipeline under:\ntools/\nâ”œâ”€â”€ picard.jar      # Required for BAM metadata tagging\nâ”œâ”€â”€ qualimap/       # For BAM quality metrics (must be Java-compatible)\nMake sure:\n\ntools/picard.jar is accessible (used by AddOrReplaceReadGroups)\ntools/qualimap/ is executable and on your PATH or referenced directly by the scripts\n\nEnsure these are available in your environment or adjust the relevant paths in the pipeline scripts.\nğŸ³ Or Use Docker For reference, installation steps are also reflected in the Dockerfiles:\nbash Copy Edit Dockerfile.pipeline1 Dockerfile.pipeline2 These show the exact installation process and can be adapted for manual setup if needed.",
    "crumbs": [
      "ğŸ Core enviroment:conda"
    ]
  },
  {
    "objectID": "wiki_combined.html#sections",
    "href": "wiki_combined.html#sections",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "2.1 ğŸ“š Sections",
    "text": "2.1 ğŸ“š Sections\n\nOverview\nPipeline 1: QC & Preprocessing\nPipeline 2: Peak Calling & Analysis\nMetadata Guide\nDocker Usage - Pipeline 1\nDocker Usage - Pipeline 2\nUnified Docker Guide\nConda Environment Setup\nSoftware Requirements\nTroubleshooting\nCitations\nLicense"
  },
  {
    "objectID": "wiki_combined.html#quick-start",
    "href": "wiki_combined.html#quick-start",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "2.2 ğŸ§ª Quick Start",
    "text": "2.2 ğŸ§ª Quick Start\nIf youâ€™re eager to run the pipeline quickly, jump to:\nğŸ‘‰ Quick Start Guide\nOr explore the Docker options for portable setup:\nğŸ‘‰ Docker Setup"
  },
  {
    "objectID": "wiki_combined.html#overview-1",
    "href": "wiki_combined.html#overview-1",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "3.1 ğŸ§¬ Overview",
    "text": "3.1 ğŸ§¬ Overview\nThis pipeline was developed and tested on Linux with POSIX-compliant behavior, macOS compatibility is not supported, If adjustments are needed for your system, you are free to modify the setup accordingly. This is the first version and still in beta test. However you can download and tested in your system as full package or use every modular script. The goal was to create a first Github project to establish Bioinformatic competence by the author.\nThis project implements a modular and reproducible ChIP-seq analysis pipeline designed for both human and mouse samples. The pipeline is organized into two main parts:\n\nPreprocessing and BAM Cleaning â€“ Modular pipeline for data preparation.\nPeak Calling and Reproducibility Analysis â€“ Peak detection, quality control, and IDR-based reproducibility assessment.\n\nThe pipeline supports both HOMER and MACS3 workflows, and uses metadata-based automation. Its structure mirrors the ENCODE ChIP-seq processing pipeline, providing a modular, standardized workflow to produce reproducible peak calls, generate signal tracks, and perform quality assessments such as IDR (Irreproducible Discovery Rate), while automating sample naming and grouping.\nThis package is intended to be flexible and modular. Each module can be customized by the user, or the pipeline can be run as-is. It is not a beginner-friendly pipeline, nor was it designed to be. The pipeline runs directly in Bash on a Linux system. Docker files are included to allow execution within containers, and a brief guide is provided at the end of this text, explaining how to build the container and mount the required folders properly.\nTo run this pipeline you will need to have your paired fastq.gz samples in the samples folder, a mapping.tsv data, and to set the reference files in the Reference folder and the spike reference genomes in the SpikeinReference folder.\nThe package provide two dataset test from GEO dataset to test Hg38 and mm10 samples.\nInformation about these two GEO datasets is provided in the README.md file inside the examples/ folder. The pipeline1 is adapted to automatically download an SRR_Acc_List.txt if the list in the metadata folder. We also provide the mapping.tsv as example"
  },
  {
    "objectID": "wiki_combined.html#pipeline-components",
    "href": "wiki_combined.html#pipeline-components",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "3.2 ğŸ“š Pipeline Components",
    "text": "3.2 ğŸ“š Pipeline Components\n\n3.2.1 ğŸ”§ 1. Read Preprocessing and Alignment\nInput: FASTQ files (reads)\nTools Used:\n\ncutadapt: Adapter trimming\nBWA: Alignment to reference genome\nSamtools, Picard: Sorting, deduplication, indexing, BAM cleanup\n\nOutputs:\n\nUnfiltered alignments\nFiltered, deduplicated BAM files\nRemoval of excluded/blacklisted regions (e.g., ENCODE blacklist)"
  },
  {
    "objectID": "wiki_combined.html#replicate-quality-control-with-deeptools",
    "href": "wiki_combined.html#replicate-quality-control-with-deeptools",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "3.3 ğŸ”¬ Replicate Quality Control with deepTools",
    "text": "3.3 ğŸ”¬ Replicate Quality Control with deepTools\nThis pipeline uses deepTools (e.g., plotFingerprint, multiBamSummary) to evaluate replicate quality.\n\n3.3.1 âœ… Advantages of deepTools:\n\nğŸ§ª Modern and actively maintained â€“ compatible with recent Python environments\nğŸ“Š Fingerprint and cross-correlation plots â€“ assess ChIP enrichment and background\nâš™ï¸ Format flexibility â€“ supports BAM and BigWig\nğŸ³ Container-friendly â€“ easy to integrate in Docker-based workflows\nğŸ“ Multi-sample analysis â€“ supports multiBamSummary, plotCorrelation\n\nUsers can optionally replace this with Relative Strand Correlation (RSC) via the ChIPQC R package by editing the relevant module â€” showcasing the pipelineâ€™s modularity.\n\n\n3.3.2 â“ Why not use phantompeakqualtools?\nAlthough once widely used to assess replicate quality, phantompeakqualtools is now:\n\nğŸ›‘ Outdated and no longer actively maintained\nğŸª Depends on legacy Perl environments and outdated tools like CClang v5\nğŸ§± Hard to containerize or integrate into modern pipelines\n\nTherefore, it has been replaced by more maintainable alternatives in this workflow.\n\n\n3.3.3 ğŸ“ˆ 2. Signal Generation\nTools:\n\nMACS3: Generates signal tracks (bedGraph or bigWig)\nBEDTools: For further manipulation\n\nOutputs:\n\nSignal p-value tracks\nFold change over control tracks\n\n\n\n3.3.4 ğŸ“ 3. Peak Calling\nPerformed on:\n\nIndividual replicates\nPooled replicates\nPseudoreplicates\n\nTools:\n\nMACS3 or HOMER\n\nOutputs:\n\nReplicated peaks\nPooled peaks\nPseudoreplicated peaks\n\n\n\n3.3.5 ğŸ“Š 4. IDR Analysis\nTool: idr â€” used to assess peak reproducibility\nThis pipeline uses a patched IDR version compatible with the latest numpy, avoiding the need to downgrade.\n\n\n3.3.6 ğŸ›  Patch + Install IDR (NumPy Compatibility)\nbash\n    wget https://github.com/kundajelab/idr/archive/refs/tags/2.0.4.2.tar.gz && \\\n    tar -xvf 2.0.4.2.tar.gz && \\\n    cd idr-2.0.4.2/idr && \\\n    sed -i 's/numpy.int/int/g' idr.py && \\\n    cd .. && \\\n    pip install . --break-system-packages && \\\n    rm -rf /opt/idr-2.0.4.2 /opt/2.0.4.2.tar.gz\nOutputs:\n\nIDR-ranked peaks\nIDR-thresholded peaks\nConservative IDR peaks\n\n\n\n3.3.7 ğŸ” 5. Replicate/Partition Concordance\nAssesses consistency of peaks across replicates and pseudoreplicates.\n\n\n3.3.8 ğŸ§¾ 6. Format Conversion and Final Outputs\nCovers conversion to final formats for visualization, downstream analysis, or submission."
  },
  {
    "objectID": "wiki_combined.html#installation-overview",
    "href": "wiki_combined.html#installation-overview",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.1 ğŸ“¦ Installation Overview",
    "text": "4.1 ğŸ“¦ Installation Overview\nThis guide explains how to set up the ChIP-seq Modular Analysis Pipeline using:\n\nâœ… Git clone (recommended)\nğŸ“ .tar.gz or .zip archive download\nğŸ³ Docker (see Docker Guide)\nğŸ§ª Conda environments (see Conda Setup)"
  },
  {
    "objectID": "wiki_combined.html#option-1-clone-from-github",
    "href": "wiki_combined.html#option-1-clone-from-github",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.2 ğŸ“‚ Option 1: Clone from GitHub",
    "text": "4.2 ğŸ“‚ Option 1: Clone from GitHub\nThe most robust and update-safe method:\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\n\n4.2.1 âœ… Make Scripts Executable (already github executable commit, but in case)\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R"
  },
  {
    "objectID": "wiki_combined.html#option-2-download-archive",
    "href": "wiki_combined.html#option-2-download-archive",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.3 ğŸ“¦ Option 2: Download Archive",
    "text": "4.3 ğŸ“¦ Option 2: Download Archive\n\n4.3.1 ğŸ”¸ If you downloaded the .tar.gz version:\ntar -xzvf your-pipeline.tar.gz\ncd your-pipeline\nScripts remain executable â€” no chmod needed.\n\n\n4.3.2 âš ï¸ If you downloaded the .zip version:\nunzip your-pipeline.zip\ncd your-pipeline\nThen run:\nchmod +x run_pipeline*.sh\nchmod +x modules/**/*.sh\nchmod +x modules/**/*.py\nchmod +x modules/**/*.R"
  },
  {
    "objectID": "wiki_combined.html#test-the-installation",
    "href": "wiki_combined.html#test-the-installation",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.4 âœ… Test the Installation",
    "text": "4.4 âœ… Test the Installation\nMake sure the scripts work:\n./run_pipeline1.sh --help\n./run_pipeline2.sh --help"
  },
  {
    "objectID": "wiki_combined.html#expected-file-structure",
    "href": "wiki_combined.html#expected-file-structure",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.5 ğŸ“ Expected File Structure",
    "text": "4.5 ğŸ“ Expected File Structure\nyour-repo/\nâ”œâ”€â”€ run_pipeline1.sh\nâ”œâ”€â”€ run_pipeline2.sh\nâ”œâ”€â”€ modules/\nâ”‚   â””â”€â”€ pipeline1/\nâ”‚   â””â”€â”€ pipeline2/\nâ”‚   â””â”€â”€ utils/\nâ”œâ”€â”€ metadata/\nâ”œâ”€â”€ Reference/\nâ”œâ”€â”€ analysis/\nâ””â”€â”€ ..."
  },
  {
    "objectID": "wiki_combined.html#related-setup-pages",
    "href": "wiki_combined.html#related-setup-pages",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.6 ğŸ“š Related Setup Pages",
    "text": "4.6 ğŸ“š Related Setup Pages\n\nğŸ“¦ Reference Setup\nğŸ§¬ Metadata & Mapping Format\nğŸ”§ Software Requirements\nğŸ³ Docker Setup\nğŸ§ª Conda Environment"
  },
  {
    "objectID": "wiki_combined.html#notes",
    "href": "wiki_combined.html#notes",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "4.7 ğŸ“ Notes",
    "text": "4.7 ğŸ“ Notes\nIf you encounter permission errors or missing dependencies, refer to Troubleshooting.\n\n\nLet me know if you want to include a section for installing a local Conda environment (`environment.yml`) or a helper `install.sh`."
  },
  {
    "objectID": "wiki_combined.html#pipeline-1-qc-preprocessing-modules",
    "href": "wiki_combined.html#pipeline-1-qc-preprocessing-modules",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "6.1 ğŸ”„ Pipeline 1: QC & Preprocessing Modules",
    "text": "6.1 ğŸ”„ Pipeline 1: QC & Preprocessing Modules"
  },
  {
    "objectID": "wiki_combined.html#supported-data-types",
    "href": "wiki_combined.html#supported-data-types",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "6.2 ğŸ§¬ Supported Data Types",
    "text": "6.2 ğŸ§¬ Supported Data Types\nThis pipeline is designed for ChIP-seq and ATAC-seq data preprocessing. It includes:\n\nAdapter trimming (cutadapt)\nParallel quality control (FastQC)\nOptional spike-in filtering\nMitochondrial read removal\nBlacklist region filtering (ENCODE)\nClean alignment with BWA and BAM post-processing\n\nBy supporting these steps, the pipeline ensures high-quality, reproducible inputs for downstream peak calling or chromatin accessibility analysis.\n\nğŸ” While optimized for ChIP-seq, the pipeline is fully compatible with ATAC-seq experiments.\n\nScripts: 01_init_directories.sh to 11_Renaming_bam.sh\nmodules/pipeline1\\$ tree . \nâ”œâ”€â”€ 01_init_directories.sh \nâ”œâ”€â”€ 02_reference_check.sh \nâ”œâ”€â”€ 03_input_fetch.sh \nâ”œâ”€â”€ 04_fastqc_parallel.sh \nâ”œâ”€â”€ 05_05_Cutadapt_trimming_phix_parallel.sh \nâ”œâ”€â”€ 06_fastqc_trimmed_parallel.sh \nâ”œâ”€â”€ 07_spike_detect.sh \nâ”œâ”€â”€ 08_alignment_bwa_spike.sh \nâ”œâ”€â”€ 09_readgroups_add.sh \nâ”œâ”€â”€ 10_bam_cleaning.sh \nâ”œâ”€â”€ 10_plot_spike_qc_summary.R \nâ””â”€â”€ 11_Renaming_bam.sh\nGreat structure! Given the flow of your Quarto document, the best place to add the usage documentation for run_pipeline1.sh would be after the ## ğŸ§¬ Supported Data Types section and before the ### ğŸ”§ Modules section.\nThis keeps your document logically organized:\n\nIntro\nWhat data it supports\nHow to run it (Usage) âœ…\nWhat each module does\nWhat the outputs are\n\n\n6.2.1 âœ… Suggested Update (add this block in your .qmd file):\n### ğŸš€ Usage: `run_pipeline1.sh`\n\nThis script is the entry point for Pipeline 1 and supports both default and customized executions.\n\n#### âœ… Default Run\n\nRun with all defaults:\n```bash\nbash run_pipeline1.sh\nThis uses:\n\nTHREADS = 4\nREFERENCE = hg38\nADAPTER = tn5_truseq\nPLATFORM = ILLUMINA\nMAPPING = metadata/mapping.tsv\nREPORT_FORMAT = human\n\n\n6.2.1.1 ğŸ”§ Full Custom Run\nExample of overriding all defaults:\nbash run_pipeline1.sh -t 8 -r mm10 -a tn5_nextera -p IONTORRENT -m data/sample_metadata.tsv -f csv\nOverrides:\n\n-t 8: 8 threads\n-r mm10: mouse genome\n-a tn5_nextera: alternative adapter\n-p IONTORRENT: sequencing platform\n-m data/sample_metadata.tsv: custom mapping file\n-f csv: report format\n\n\n\n6.2.1.2 â„¹ï¸ Help\nFor option details:\nbash run_pipeline1.sh --help\n\n\n\n6.2.2 ğŸ”§ Modules\n\nğŸ”§ Modules Overview\n\n\nScript\nPurpose\n\n\n\n\n01_init_directories.sh\nCreates necessary directory structure.\n\n\n02_reference_check.sh\nValidates presence of genome reference files.\n\n\n03_input_fetch.sh\nFetches and decompresses user-supplied FASTQ files.\n\n\n04_fastqc_parallel.sh\nRuns FastQC and generates quality reports.\n\n\n05_Cutadapt_trimming_phix_parallel.sh\nTrims adapters and low-quality bases using Cutadapt.\n\n\n06_fastqc_trimmed_parallel.sh\nRuns FastQC on trimmed reads.\n\n\n07_spike_detect.sh\nDetects exogenous spike-in if not provided.\n\n\n08_alignment_bwa_spike.sh\nAligns reads using BWA and checks spike content.\n\n\n09_readgroups_add.sh\nAdds read group metadata using Picard.\n\n\n10_bam_cleaning.sh\nCleans and indexes BAMs; performs spike-in QC.\n\n\n11_Renaming_bam.sh\nRenames BAMs using mapping.tsv for grouping.\n\n\n\n\n\n6.2.3 ğŸ“ Output Folders\nresults/: Raw outputs and intermediate files.\nanalysis/Renamed_Cleaned: Cleaned and filtered BAMs for downstream usage.\nresults/\nresults/Filtered$ tree \n\nâ”œâ”€â”€ BAM}\nâ”œâ”€â”€ Filtered\nâ”œ        â”œâ”€â”€ Cleaned  âœ… here are the cleaned BAM that will be renamed. â”‚\nâ”œ        â”œâ”€â”€ Deduplicated\nâ”œ        â””â”€â”€ Metrics\nâ”œâ”€â”€ QC_fastqc\nâ”œâ”€â”€ QC_spike_plots\nâ”œâ”€â”€ QC_trimmed_fastqc\nâ”œâ”€â”€ spike_analysis\nâ””â”€â”€ Trimmed"
  },
  {
    "objectID": "wiki_combined.html#functional-enrichment-report",
    "href": "wiki_combined.html#functional-enrichment-report",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "7.1 ğŸ§¬ Functional Enrichment Report",
    "text": "7.1 ğŸ§¬ Functional Enrichment Report\nThe final enrichment summary report (including Gene Ontology, KEGG, Reactome, and g:Profiler figures) is available as a downloadable PDF and HTML. If the PRE_POSSIBLE_OUTCOME.tsv file is too large, the PDF will include only the g:Profiler figures. The full PRE_POSSIBLE_OUTCOME.tsv content will remain available in its original TSV format and within the HTML report.\n\n7.1.1 ğŸ“ Example Reports\nğŸ‘‰ hg38: ğŸ“ View report (PDF)\nğŸ‘‰ mm10: ğŸ“ View report (PDF)\n\n\n7.1.2 ğŸš€ Usage: run_pipeline2.sh\nThis script runs Pipeline 2 â€” performing peak calling, replicate QC, IDR analysis, and functional annotation using MACS3 and HOMER.\n\n7.1.2.1 âœ… Default Run\nRun with all default parameters:\nbash run_pipeline2.sh\nThis will use:\n\nREFERENCE = hg38\nMAPPING = metadata/mapping.tsv\nPEAK_CALLER = both (MACS3 and HOMER)\nTHREADS = 4\nIDR_THRESHOLD = 0.05\nTARGET_RULES = templates/target_peak_rules.yaml\nTSS_DB = HOMER (for annotation)\n\n\nâœ… Defaults are suitable for most ChIP-seq and ATAC-seq use cases.\n\n\n\n7.1.2.2 ğŸ”§ Full Custom Run\nCustomize all parameters:\nbash run_pipeline2.sh \\\n  -r mm10 \\\n  -m data/sample_metadata.tsv \\\n  -p macs3 \\\n  -t 8 \\\n  -i 0.01 \\\n  -y templates/target_peak_rules_HOMER.yaml \\\n  --tss-db clusterProfiler\nExplanation:\n\n-r mm10 â†’ Use mouse genome\n-m data/sample_metadata.tsv â†’ Custom metadata file\n-p macs3 â†’ Use MACS3 only (can be macs3, homer, or both)\n-t 8 â†’ Use 8 threads\n-i 0.01 â†’ Adjust IDR threshold\n-y templates/target_peak_rules_HOMER.yaml â†’ Custom target rules\n--tss-db clusterProfiler â†’ Use clusterProfiler instead of HOMER for gene annotation.\n\n\n\n7.1.2.3 â„¹ï¸ Help\nTo see all options:\nbash run_pipeline2.sh --help\n\n\n\n7.1.3 ğŸ“„ 1. mapping_schema.yaml â€” Metadata Validation Rules\nDefines structure and validation logic for your mapping.tsv metadata.\n\n7.1.3.1 ğŸ” Key Elements:\n\nRequired fields: Sample_ID, Instrument, Sample_Type, Condition, Replicate, Target\nOptional fields: Cell_line, Spike_Type\nRegex validation: For fields like Sample_Type, Replicate\n\n\n\n7.1.3.2 âœ… Validate Schema File\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\n\n7.1.3.3 âœ… Validate Metadata File\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\n\n\n\n7.1.4 ğŸ“„ 2. target_peak_rules.yaml â€” Peak Calling Styles (MACS3)\nThis config determines which targets require narrow or broad peak calling.\nExamples:\n\nCTCF, MYC â†’ narrowPeak\nH3K27me3, EZH2 â†’ broadPeak\n\n\n\n7.1.5 ğŸ“„ 3. target_peak_rules_HOMER.yaml â€” Peak Calling Styles (HOMER)\nClassifies targets by style for findPeaks:\n\nfactor â†’ e.g., MYC, SOX2\nhistone â†’ e.g., H3K27me3, EZH2\nGROseq, DNaseI, etc.\n\n\nâš ï¸ Note: These files are pre-validated. If you add new targets, you must write your own validation."
  },
  {
    "objectID": "wiki_combined.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "href": "wiki_combined.html#modules-01_replicate_qc.sh-to-06_igv_snapshot.sh",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "7.2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh",
    "text": "7.2 ğŸ“Š Modules: 01_replicate_qc.sh to 06_igv_snapshot.sh\nmodules/pipeline2/\nâ”œâ”€â”€ 01_replicate_qc.sh\nâ”œâ”€â”€ 02_merge_pseudoreplicates.sh\nâ”œâ”€â”€ 03_1_MACS3_peak_calling.sh\nâ”œâ”€â”€ 03_2_homer_peak_calling_fold_fdr_relaxed.sh\nâ”œâ”€â”€ 03_2_homer_to_peakFormat.R\nâ”œâ”€â”€ 03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 03_4_homer_peak_calling_pooled_pseudoreps.sh\nâ”œâ”€â”€ 04_plot_idr_summary.R\nâ”œâ”€â”€ 04_run_idr.sh\nâ”œâ”€â”€ 05_peak_annotation.sh\nâ”œâ”€â”€ 06_igv_snapshot.sh\nâ””â”€â”€ cluster_enrichment_updated_hg_mice.R\n\n7.2.1 ğŸ”§ Modules\n\n\n\n\n\n\n\nScript\nPurpose\n\n\n\n\n01_replicate_qc.sh\nAnalyzes biological replicates for correlation (cutoff: 0.8), filters low-correlation samples.\n\n\n02_merge_pseudoreplicates.sh\nMerges BAMs and creates pseudo-replicates for IDR.\n\n\n03_1_MACS3_peak_calling.sh\nPeak calling using MACS3 with smart baseline control matching.\n\n\n03_2_homer_peak_calling_fold_fdr_relaxed.sh\nHOMER-based peak calling using target_peak_rules_HOMER.yaml.\n\n\n03_3_MACS3_peak_calling_pooled_pseudoreps.sh\nMACS3 peak calling for pooled/pseudo BAMs.\n\n\n03_4_homer_peak_calling_pooled_pseudoreps.sh\nHOMER peak calling for pooled/pseudo BAMs.\n\n\n04_run_idr.sh\nRuns IDR on replicate pairs, pooled vs individual, etc.\n\n\n05_peak_annotation.sh\nPerforms TSS annotation of peak files.\n\n\n06_igv_snapshot.sh\nGenerates IGV snapshots for peak visualization.\n\n\n\n\n\n\n\n\nâš ï¸ Note :06_igv_snapshot.sh is an extra module not yet adapted to the pipeline. This particular script produce to many snapshots if the user do not define the peaks based on their needs. â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”"
  },
  {
    "objectID": "wiki_combined.html#analysis-folder-structure",
    "href": "wiki_combined.html#analysis-folder-structure",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "7.3 ğŸ—‚ Analysis Folder Structure",
    "text": "7.3 ğŸ—‚ Analysis Folder Structure\nanalysis$ tree -L 1\n.\nâ”œâ”€â”€ BAM_replicate_fail\nâ”œâ”€â”€ ChIPseeker_TSS_Hommer_IDR_annotation\nâ”œâ”€â”€ IDR_Results\nâ”‚   â”œâ”€â”€ homer\nâ”‚   â””â”€â”€ macs3\nâ”œâ”€â”€ PeakCalling_HOMER\nâ”œâ”€â”€ PeakCalling_HOMER_pool_pseudo\nâ”œâ”€â”€ PeakCalling_MACS3\nâ”œâ”€â”€ PeakCalling_MACS3_pool_pseudo\nâ”œâ”€â”€ Pooled_BAMs\nâ”œâ”€â”€ pooling_log.tsv\nâ”œâ”€â”€ Pool_Pseudo_QC_stats\nâ”œâ”€â”€ Pseudoreplicates\nâ”œâ”€â”€ Renamed_Cleaned\nâ””â”€â”€ Replicate_QC\n    â”œâ”€â”€ bigwig\n    â”œâ”€â”€ deeptools\n    â”œâ”€â”€ pbc_metrics.tsv\n    â””â”€â”€ tmp_groups"
  },
  {
    "objectID": "wiki_combined.html#creating-the-mapping.tsv-file-metadata-table",
    "href": "wiki_combined.html#creating-the-mapping.tsv-file-metadata-table",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "8.1 ğŸ—ºï¸ Creating the mapping.tsv File (Metadata Table)",
    "text": "8.1 ğŸ—ºï¸ Creating the mapping.tsv File (Metadata Table)\nThe metadata file is crucial for automating sample grouping, identifying replicates, and selecting the appropriate peak caller.\n\nğŸ§  Filename required: metadata/mapping.tsv\n\n\n8.1.1 âœ… Required Columns\n\n\n\n\n\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nSample_ID\nUnique ID matching FASTQ or BAM files\nSRR123456\n\n\nInstrument\nSequencing instrument or platform\nIllumina\n\n\nSample_Type\nChIP, Input, IgG, Mock, etc.\nChIP\n\n\nCondition\nExperimental condition or group\nTreated, WT\n\n\nReplicate\nReplicate number (1, 2, 3, â€¦)\n1\n\n\nTarget\nTranscription factor or histone mark\nH3K27ac, CTCF\n\n\n\n\n\n8.1.2 ğŸ“ Optional Columns\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nCell_line\nCell line used\nHEK293\n\n\nSpike_Type\nIf spike-in is used (e.g., dm6, ERCC)\ndm6\n\n\n\n\n\n8.1.3 ğŸ“ Example Structure\nSample_ID   Instrument  Sample_Type Condition   Replicate   Target  Cell_line   Spike_Type\nSRR001      Illumina    ChIP        WT          1         CTCF       HEK293     dm6\nSRR002      Illumina    ChIP        WT          2         CTCF       HEK293     dm6\nSRR003      Illumina    Input       WT          1         None       HEK293     dm6\nSRR004      Illumina    ChIP        KO          1         H3K27me3   HEK293     dm6\nSRR005      Illumina    ChIP        KO          2         H3K27me3   HEK293     dm6\nSRR006      Illumina    Input       KO          1         None       HEK293     dm6\n\n\n8.1.4 ğŸ§ª Validation Steps\nMake sure your file is valid before starting the pipeline:\n\nValidate schema:\n\nbash modules/utils/validate_mapping_yaml.sh templates/mapping_schema.yaml\n\nValidate metadata:\n\npython3 modules/utils/validate_mapping.py \\\n  --mapping metadata/mapping.tsv \\\n  --schema templates/mapping_schema.yaml\nâœ”ï¸ You should see â€œValidation Passedâ€ if everything is correct."
  },
  {
    "objectID": "wiki_combined.html#what-is-mapping.tsv-and-why-is-it-important",
    "href": "wiki_combined.html#what-is-mapping.tsv-and-why-is-it-important",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "8.2 ğŸ“‹ What is mapping.tsv and Why Is It Important?",
    "text": "8.2 ğŸ“‹ What is mapping.tsv and Why Is It Important?\n\n8.2.1 ğŸ” What is it?\nThe mapping.tsv is a metadata tableâ€”a plain text file in tab-separated formatâ€”that contains structured information about each sample in your ChIP-seq experiment. Each row represents a sequencing sample, and each column provides key attributes (e.g., sample ID, condition, replicate number, target protein, etc.).\nThis file needs to be stored at:\nmetadata/mapping.tsv\nThis mapping table will be transformed during the pipeline process: at then you will have a backup (mapping.tsv.bak), mapping_filtered.tsv (produce after replicates QC), and mapping_scaled.tsv (produced after bam cleaning, it contains the ratio of exogenous spike , if spike was detected, and the spike genome type).\n\n\n8.2.2 ğŸ§  Why is it needed?\nThis file is critical for enabling your pipeline to:\nâœ… Recognize and organize samples automatically âœ… Group replicates (for IDR and reproducibility checks) âœ… Assign controls (Input, IgG, Mock) correctly for peak calling âœ… Select peak caller styles (e.g., narrowPeak for TFs, broadPeak for histone marks) âœ… Validate input consistency using the YAML schema\nWithout this file, the pipeline wouldnâ€™t know how your samples relate to each otherâ€”or how to process them correctly.\n\n\n8.2.3 ğŸ”— How does the pipeline use it?\n\nPipeline 1: The script 11_Renaming_bam.sh uses the metadata to automatically rename BAM files and organize them into logical groups.\nPipeline 2: Several scripts (e.g., 01_replicate_qc.sh, 03_1_MACS3_peak_calling.sh) read mapping.tsv to apply the right rules for:\n\nSample pairing (ChIP vs.Â Input)\nReplicate merging\nPeak calling type\nReproducibility evaluation (IDR)\n\nValidation scripts (validate_mapping.py, validate_mapping_yaml.sh) ensure the file is complete and follows strict format rules before any analysis starts.\n\n\n\n8.2.4 ğŸ§¬ Summary\n\n\n\n\n\n\n\nFeature\nWhy it matters\n\n\n\n\nAutomation\nRemoves manual handling of sample groups\n\n\nReproducibility\nEnsures the same logic applies every time\n\n\nCompatibility\nLets the pipeline work across diverse experimental designs\n\n\nQuality control\nPrevents broken analyses due to misannotated samples"
  },
  {
    "objectID": "wiki_combined.html#running-pipeline1-with-docker",
    "href": "wiki_combined.html#running-pipeline1-with-docker",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "9.1 ğŸƒ Running Pipeline1 with Docker",
    "text": "9.1 ğŸƒ Running Pipeline1 with Docker\nğŸ³ Docker Usage Guide\nThis guide explains how to build and run the ChIP-seq processing pipeline using Docker. It is designed for users at all levels, including beginners.\nWhile the original pipeline was created to work directly in Host-based installation, the pipeline can be used with docker containers and mounting the required folder needed for the package This pipeline is fully containerized and can be run with a single Docker command. Below is the recommended usage to preserve outputs, access your own data, and avoid re-downloading references.\n\n9.1.1 ğŸ”§ Docker Requirements\n\nDocker installed and working (docker --version)\nYour working directory should contain:\n\nReference/ â€” genome files (e.g.Â hg38, mm10, etc.)\nmetadata/ â€” sample sheet (SRR_Acc_List.txt, etc.)\nsamples/ â€” (optional) pre-existing FASTQs\nresults/ â€” (created if missing) final output goes here\nlogs/ â€” (created if missing) all logs go here\n\n\n\n\n9.1.2 ğŸ§ª Run Command (Example)\ndocker run --rm \\\n  -v \"$PWD/Reference\":/pipeline/Reference \\\n  -v \"$PWD/SpikeinReference\":/pipeline/SpikeinReference \\\n  -v \"$PWD/metadata\":/pipeline/metadata \\\n  -v \"$PWD/samples\":/pipeline/samples \\\n  -v \"$PWD/results\":/pipeline/results \\\n  -v \"$PWD/logs\":/pipeline/logs \\\n  -v \"$PWD/logs\":/pipeline/assets \\\n  pipeline1:latest -t 4 -r mm10 -a tn5_truseq\n\n\n9.1.3 ğŸ“ Explanation\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--rm\nRemoves container after run\n\n\n-v &lt;host&gt;:&lt;container&gt;\nMounts folders into container\n\n\n-t 4\nNumber of threads\n\n\n-r mm10\nReference genome (hg38, mm10, etc.)\n\n\n-a tn5_truseq\nAdapter type (should match whatâ€™s in your pipeline)\n\n\npipeline1:latest\nDocker image name\n\n\n\n\n\n9.1.4 ğŸ“‚ Output\nAll outputs go into your local results/ and logs/ folders â€” no data is lost after the container exits."
  },
  {
    "objectID": "wiki_combined.html#software-requirements-non-docker-setup",
    "href": "wiki_combined.html#software-requirements-non-docker-setup",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "12.1 âš™ï¸ Software Requirements (Non-Docker Setup)",
    "text": "12.1 âš™ï¸ Software Requirements (Non-Docker Setup)\nWhile this pipeline is fully Docker-compatible, you can also run it natively by installing the required software through ğŸ Conda and ğŸ“˜ â„ .\n\n12.1.1 ğŸ“ Environment Files\nAll environment setup files are located in:\nenv/\nâ”œâ”€â”€ env_pipeline_base.yml     # Conda YAML file (base system + CLI tools)\nâ”œâ”€â”€ env_pipeline_r.R          # R script to install required R/Bioconductor packages\n\n\n12.1.2 ğŸ§ª Step 1: Create Conda Environment\nUse the provided YAML file to create a clean Conda environment with all the command-line tools.\nconda env create -f env/env_pipeline_base.yml\nconda activate chipseq_pipeline_base\n\n\n12.1.3 ğŸ“¦ Step 2: Install R and Bioconductor Packages\nOnce inside the conda environment, install the required R packages using:\nRscript env/env_pipeline_r.R\nThis script will:\n\nCheck for missing CRAN or Bioconductor dependencies\nInstall them as needed\nLoad the correct annotation packages for both human (hg38) and mouse (mm10) genomes\n\n\n\n12.1.4 ğŸ§° Tool Dependencies\nSome tools like picard.jar and qualimap are not installed via Conda but are required. These are bundled inside the pipeline under:\ntools/\nâ”œâ”€â”€ picard.jar      # Required for BAM metadata tagging\nâ”œâ”€â”€ qualimap/       # For BAM quality metrics (must be Java-compatible)\nMake sure:\n\ntools/picard.jar is accessible (used by AddOrReplaceReadGroups)\ntools/qualimap/ is executable and on your PATH or referenced directly by the scripts\n\nEnsure these are available in your environment or adjust the relevant paths in the pipeline scripts.\nğŸ³ Or Use Docker For reference, installation steps are also reflected in the Dockerfiles:\nbash Copy Edit Dockerfile.pipeline1 Dockerfile.pipeline2 These show the exact installation process and can be adapted for manual setup if needed."
  },
  {
    "objectID": "wiki_combined.html#software-requirements-non-docker-setup-1",
    "href": "wiki_combined.html#software-requirements-non-docker-setup-1",
    "title": "ChIP-seq Modular Analysis Pipeline",
    "section": "13.1 ğŸ§° Software Requirements (Non-Docker Setup)",
    "text": "13.1 ğŸ§° Software Requirements (Non-Docker Setup)\nIf youâ€™re running the pipeline without Docker, your system must have the following tools installed and available in your PATH â€” except for picard.jar and qualimap, which are already bundled in the pipeline under tools/ and should remain there.\n\n13.1.1 ğŸ“¦ Core Requirements\nInstall the following tools via your package manager (apt, brew, conda, etc.) or from source as needed:\n\n\n\n\n\n\n\n\nTool\nMinimum Version\nPurpose\n\n\n\n\nbash\n4.0+\nShell scripting\n\n\ncutadapt\n4.0+\nAdapter trimming\n\n\nbwa\n0.7.17+\nRead alignment\n\n\nsamtools\n1.14+\nBAM/SAM manipulation\n\n\npicard\n2.26.10+\nBAM post-processing\n\n\nmacs3\n3.0.0a6+\nPeak calling\n\n\nhomer\nv4.11+\nAlternative peak calling\n\n\npython3\n3.9+\nUsed for helper scripts\n\n\npip\nLatest\nPython package management\n\n\nR\n4.2+\nStatistical computing environment\n\n\nRscript\n4.2+\nScript execution for R modules\n\n\nbedtools\n2.30.0+\nGenomic interval operations\n\n\ndeepTools\n3.5.1+\nQC and coverage tools\n\n\nidr\n2.0.4.2 (patched)\nPeak reproducibility scoring\n\n\njq\n1.6+\nJSON parsing (metadata scripts)\n\n\nwget/curl\nany\nData downloading\n\n\nyq\n4.0+\nYAML parsing and editing (Go-based)\n\n\n\n\n\n13.1.2 ğŸ“š Required Python Packages\nInstall with:\npip install numpy pandas pyyaml\n\n\n13.1.3 ğŸ“š Required R Packages\nYour pipeline includes scripts that require the following CRAN and Bioconductor packages:\n\n13.1.3.1 From CRAN:\n\ndplyr\nggplot2\ntidyr\ndata.table\nreadr\nstringr\ntools\njsonlite\nhttr\n\n\n\n13.1.3.2 From Bioconductor:\n\nclusterProfiler\nenrichplot\nReactomePA\nreactome.db\nbiomaRt\nAnnotationDbi\norg.Hs.eg.db\norg.Mm.eg.db\n\n\nâœ… The R scripts automatically attempt to install missing packages if your internet connection is available.\n\n\n\n\n13.1.4 ğŸ§¬ Optional (for full compatibility)\n\nFastQC\nMultiQC\nIGV (for manual visualization)\ndocker (if youâ€™d like to use pre-built containers)"
  }
]